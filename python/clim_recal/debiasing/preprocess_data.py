#!/bin/python3

# Script to pre-process control, scenario and observation data (including combining files to cover a range of dates),
# before running debiasing methods.

import argparse
import glob
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np

#sys.path.insert(1, "../load_data")
from clim_recal.data_loader import load_data

# * ----- L O G G I N G -----
formatter = logging.Formatter(
    fmt="%(asctime)s %(module)s,line: %(lineno)d %(levelname)8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

log = logging.getLogger()
log.setLevel(logging.INFO)
screen_handler = logging.StreamHandler(stream=sys.stdout)
screen_handler.setFormatter(formatter)
logging.getLogger().addHandler(screen_handler)

# * ----- I N P U T - H A N D L I N G -----
parser = argparse.ArgumentParser(description="Pre-process data before bias correction.")
parser.add_argument(
    "--mod",
    "--modelled",
    dest="mod_fpath",
    type=str,
    help="Path to modelled (CPM) datasets",
)
parser.add_argument(
    "--obs",
    "--observed",
    dest="obs_fpath",
    type=str,
    help="Path to observation (HADs) datasets",
)
parser.add_argument(
    "--calib_dates",
    "--calibration_date_range",
    dest="calibration_date_range",
    type=str,
    help="Start and end dates for calibration (historic CPM/HADs data used to "
    "calibrate the debiasing model) - in YYYYMMDD-YYYYMMDD format",
    default="19801201-19991130",
)
parser.add_argument(
    "--valid_dates",
    "--validation_date_range",
    dest="validation_date_range",
    type=str,
    help="Start and end dates for validation data (CPM data to be debiased using the "
    "calibrated debiasing model, and HADs data as referece) - multiple date ranges can be passed, "
    'separated by "_", each in YYYYMMDD-YYYYMMDD format e.g., '
    '"20100101-20191231_20200101-20291231"',
    default="20201201-20291130_20301201-20391130",
)
parser.add_argument(
    "--shp",
    "--shapefile",
    dest="shapefile_fpath",
    type=str,
    help="Path to shapefile",
    default=None,
)
parser.add_argument(
    "--out",
    "--output",
    dest="output_fpath",
    type=str,
    help="Path to save output files",
    default=".",
)
parser.add_argument(
    "-v",
    "--variable",
    dest="var",
    type=str,
    default="tasmax",
    help="Variable to adjust",
)
parser.add_argument(
    "-u", "--unit", dest="unit", type=str, default="Â°C", help="Unit of the varible"
)
parser.add_argument(
    "-r",
    "--run_number",
    dest="run_number",
    type=str,
    default=None,
    help="Run number to process (out of 13 runs in the CPM data)",
)

params = vars(parser.parse_args())

obs_fpath = params["obs_fpath"]
mod_fpath = params["mod_fpath"]
calibration_date_range = params["calibration_date_range"]
validation_date_range = params["validation_date_range"]
shape_fpath = params["shapefile_fpath"]
out_fpath = params["output_fpath"]
var = params["var"]
unit = params["unit"]
run_number = params["run_number"]

calib_list = calibration_date_range.split("-")
h_date_period = (
    datetime.strptime(calib_list[0], "%Y%m%d").strftime("%Y-%m-%d"),
    datetime.strptime(calib_list[1], "%Y%m%d").strftime("%Y-%m-%d"),
)
val_list = validation_date_range.split("_")
future_time_periods = [(p.split("-")[0], p.split("-")[1]) for p in val_list]
future_time_periods = [
    (
        datetime.strptime(p[0], "%Y%m%d").strftime("%Y-%m-%d"),
        datetime.strptime(p[1], "%Y%m%d").strftime("%Y-%m-%d"),
    )
    for p in future_time_periods
]


# * ----- ----- -----M A I N ----- ----- -----
def preprocess_data() -> None:
    start = time.time()

    # load every file found with extension in the path and selects only the input time period.from
    # coordinates are renamed for compatibility with the cmethods-library
    use_pr = False
    if var == "rainfall":
        use_pr = True

    # load modelled data (CPM) for calibration period and place into ds_modc
    if run_number is not None:
        ds_modc = load_data(
            mod_fpath,
            date_range=h_date_period,
            variable=var,
            filter_filenames_on_variable=True,
            run_number=run_number,
            filter_filenames_on_run_number=True,
            use_pr=use_pr,
            shapefile_path=shape_fpath,
            extension="tif",
        )[var].rename(
            {"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"}
        )
    else:
        ds_modc = load_data(
            mod_fpath,
            date_range=h_date_period,
            variable=var,
            filter_filenames_on_variable=True,
            use_pr=use_pr,
            shapefile_path=shape_fpath,
            extension="tif",
        )[var].rename(
            {"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"}
        )

    # find file extensions for observation data
    files_obs_nc = glob.glob(f"{obs_fpath}/*.nc", recursive=True)
    files_obs_tif = glob.glob(f"{obs_fpath}/*.tif", recursive=True)

    if len(files_obs_nc) > 0 and len(files_obs_tif) == 0:
        ext = "nc"
    elif len(files_obs_nc) == 0 and len(files_obs_tif) > 0:
        ext = "tif"
    elif len(files_obs_nc) == 0 and len(files_obs_tif) == 0:
        raise Exception(
            f"No observation files found in {obs_fpath} with extensions .nc or .tif"
        )
    else:
        raise Exception(
            f"A mix of .nc and .tif observation files found in {obs_fpath}, file extension should be the "
            f"same for all files in the directory."
        )

    # load observation data (HADs) for calibration period and place into ds_obsc
    ds_obsc = load_data(
        obs_fpath,
        date_range=h_date_period,
        variable=var,
        filter_filenames_on_variable=True,
        shapefile_path=shape_fpath,
        extension=ext,
    )[var].rename({"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"})
    log.info("Calibration data (modelled and observed) loaded.")

    # aligning calendars, there might be extra days in the modelled data that need to be dropped
    ds_modc = ds_modc.sel(time=ds_obsc.time, method="nearest")

    if ds_obsc.shape != ds_modc.shape:
        raise RuntimeError(
            "Error, observed and modelled calibration data must have same dimensions."
        )

    log.info("Resulting calibration datasets with shape")
    log.info(ds_obsc.shape)

    # masking coordinates where the observed data has no x, y values
    ds_modc = ds_modc.where(~np.isnan(ds_obsc.isel(time=0)))
    ds_modc = ds_modc.where(ds_modc.values < 1000)
    log.info("Calibration data masked")

    ds_obsc.attrs["unit"] = unit
    ds_modc.attrs["unit"] = unit

    # write modc to .nc file in output directory
    modc_filename = f"modc_var-{var}_run-{run_number}_{calib_list[0]}_{calib_list[1]}"
    modc_path = os.path.join(out_fpath, f"{modc_filename}.nc")
    if not os.path.exists(os.path.dirname(modc_path)):
        folder_path = Path(os.path.dirname(modc_path))
        folder_path.mkdir(parents=True)
    print(f"Saving modelled (CPM) data for calibration to {modc_path}")
    ds_modc.to_netcdf(modc_path)
    log.info(f"Saved modelled (CPM) data for calibration to {modc_path}")

    # write ds_obsc to .nc file in output directory
    obsc_filename = f"obsc_var-{var}_run-{run_number}_{calib_list[0]}_{calib_list[1]}"
    obsc_path = os.path.join(out_fpath, f"{obsc_filename}.nc")
    if not os.path.exists(os.path.dirname(obsc_path)):
        folder_path = Path(os.path.dirname(obsc_path))
        folder_path.mkdir(parents=True)
    print(f"Saving observation data (HADs) for calibration to {obsc_path}")
    ds_obsc.to_netcdf(obsc_path)
    log.info(f"Saved observation data (HADs) for calibration period to {obsc_path}")

    # looping over validation time periods
    for f_date_period in future_time_periods:
        log.info(f"Running for {f_date_period} time period")

        # load modelled (CPM) data for validation period and store in ds_modv
        try:
            use_pr = False
            if var == "rainfall":
                use_pr = True

            # load
            if run_number is not None:
                ds_modv = load_data(
                    mod_fpath,
                    date_range=f_date_period,
                    variable=var,
                    run_number=run_number,
                    filter_filenames_on_run_number=True,
                    use_pr=use_pr,
                    shapefile_path=shape_fpath,
                    filter_filenames_on_variable=True,
                    extension="tif",
                )[var].rename(
                    {"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"}
                )
            else:
                ds_modv = load_data(
                    mod_fpath,
                    date_range=f_date_period,
                    variable=var,
                    filter_filenames_on_variable=True,
                    use_pr=use_pr,
                    shapefile_path=shape_fpath,
                    extension="tif",
                )[var].rename(
                    {"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"}
                )
        except Exception as e:
            log.info(f"No modelled data available for {f_date_period} time period")
            continue

        # load observed (HADs) data for validation period and store in ds_obsv
        try:
            ds_obsv = load_data(
                obs_fpath,
                date_range=f_date_period,
                variable=var,
                filter_filenames_on_variable=True,
                shapefile_path=shape_fpath,
                extension=ext,
            )[var].rename(
                {"projection_x_coordinate": "lon", "projection_y_coordinate": "lat"}
            )
        except Exception as e:
            log.info(f"No observed data available for {f_date_period} time period")
            continue

        # aligning calendars, there might be extra days in the modelled data that need to be dropped
        ds_modv = ds_modv.sel(time=ds_obsv.time, method="nearest")

        if ds_obsv.shape != ds_modv.shape:
            raise RuntimeError(
                "Error, observed and modelled validation data must have same dimensions."
            )

        log.info("Resulting validation datasets with shape")
        log.info(ds_obsv.shape)

        # masking coordinates where the observed data has no x, y values
        ds_modv = ds_modv.where(~np.isnan(ds_obsv.isel(time=0)))
        ds_modv = ds_modv.where(ds_modv.values < 1000)

        ds_obsv.attrs["unit"] = unit
        ds_modv.attrs["unit"] = unit

        # write ds_modv and ds_obsv to .nc files in output directory
        ds_modv_filename = f'modv_var-{var}_run-{run_number}_{f_date_period[0].replace("-","")}_{f_date_period[1].replace("-","")}'
        ds_obsv_filename = f'obsv_var-{var}_run-{run_number}_{f_date_period[0].replace("-","")}_{f_date_period[1].replace("-","")}'
        ds_modv_path = os.path.join(out_fpath, f"{ds_modv_filename}.nc")
        ds_obsv_path = os.path.join(out_fpath, f"{ds_obsv_filename}.nc")
        if not os.path.exists(os.path.dirname(ds_modv_path)):
            folder_path = Path(os.path.dirname(ds_modv_path))
            folder_path.mkdir(parents=True)
        if not os.path.exists(os.path.dirname(ds_obsv_path)):
            folder_path = Path(os.path.dirname(ds_obsv_path))
            folder_path.mkdir(parents=True)
        print(f"Saving modelled (CPM) data for validation to {ds_modv_path}")
        ds_modv.to_netcdf(ds_modv_path)
        log.info(
            f"Saved modelled (CPM) data for validation, period {f_date_period} to {ds_modv_path}"
        )
        print(f"Saving observed (HADs) data for validation to {ds_obsv_path}")
        ds_obsv.to_netcdf(ds_obsv_path)
        log.info(
            f"Saved observed (HADs) data for validation, period {f_date_period} to {ds_modv_path}"
        )

    end = time.time()
    log.info(f"total time in seconds: {end - start}")
    log.info("Done")


if __name__ == "__main__":
    preprocess_data()

# * ----- ----- E O F ----- -----
