[
  {
    "objectID": "docs/reference/clim_recal.ceda_ftp_download.html",
    "href": "docs/reference/clim_recal.ceda_ftp_download.html",
    "title": "1 clim_recal.ceda_ftp_download",
    "section": "",
    "text": "clim_recal.ceda_ftp_download\n\n\n\n\n\nName\nDescription\n\n\n\n\ndownload_ftp\nFunction to connect to the CEDA archive and download data.\n\n\n\n\n\nclim_recal.ceda_ftp_download.download_ftp(input, output, username, password, order)\nFunction to connect to the CEDA archive and download data.\nYou need to have a user account and provide your username and FTP password.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired",
    "crumbs": [
      "API Reference",
      "CEDA Data Access"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.ceda_ftp_download.html#functions",
    "href": "docs/reference/clim_recal.ceda_ftp_download.html#functions",
    "title": "1 clim_recal.ceda_ftp_download",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndownload_ftp\nFunction to connect to the CEDA archive and download data.\n\n\n\n\n\nclim_recal.ceda_ftp_download.download_ftp(input, output, username, password, order)\nFunction to connect to the CEDA archive and download data.\nYou need to have a user account and provide your username and FTP password.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired",
    "crumbs": [
      "API Reference",
      "CEDA Data Access"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html",
    "href": "docs/reference/clim_recal.utils.core.html",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "clim_recal.utils.core\nUtility functions.\n\n\n\n\n\nName\nDescription\n\n\n\n\nMonthDay\nA class to ease generating annual time series.\n\n\n\n\n\nclim_recal.utils.core.MonthDay(self, month=1, day=1, format_str=ISO_DATE_FORMAT_STR)\nA class to ease generating annual time series.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmonth\nint\nWhat Month as an integer from 1 to 12.\n\n\nday\nint\nWhat day in the month, an integer from 1 to 31.\n\n\nformat_str\nstr\nFormat to use if generating a str.\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year(1980)\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980')\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980', as_str=True)\n'1980-01-01'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_year\nReturn a date or str of date given year.\n\n\nfrom_year_range_to_str\nReturn an annual range str.\n\n\n\n\n\nclim_recal.utils.core.MonthDay.from_year(year, as_str=False)\nReturn a date or str of date given year.\n\n\n\nclim_recal.utils.core.MonthDay.from_year_range_to_str(start_year, end_year, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nReturn an annual range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nend_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nsplit_str\nstr\nstr between start_date and end_date generated.\nDATE_FORMAT_SPLIT_STR\n\n\ninclude_end_date\nbool\nWhether to include the end_date in the final str. If False follow python convention to return the day prior.\nTrue\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year_range_to_str(1980, 1982, include_end_date=False)\n'19800101-19811231'\n&gt;&gt;&gt; jan_1.from_year_range_to_str('1980', 2020)\n'19800101-20200101'\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nannual_data_path\nReturn Path of annual data files.\n\n\nannual_data_paths_generator\nYield Path of annual data files.\n\n\ncheck_package_path\nReturn path for test running.\n\n\nclimate_data_mount_path\nReturn likely climate data mount path based on operating system.\n\n\ncsv_reader\nYield a dict per row from a CSV file at path.\n\n\ndate_range_generator\nReturn a tuple of DateType objects.\n\n\ndate_range_to_str\nTake start_date and end_date and return a date range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\nensure_date\nEnsure passed date_to_check is a date.\n\n\nis_climate_data_mounted\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nis_platform_darwin\nCheck if sys.platform is Darwin (macOS).\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmultiprocess_execute\nRun method_name as from iter via multiprocessing.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nproduct_dict\nReturn product combinatorics of kwargs.\n\n\nrange_len\nCacluate the total length of range with indexing.\n\n\nresults_path\nReturn Path: path/name_time.extension.\n\n\nrun_callable_attr\nExtract method_name from instance to call.\n\n\ntime_str\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\nclim_recal.utils.core.annual_data_path(start_year=1980, end_year=1981, month_day=DEFAULT_CPM_START_MONTH_DAY, include_end_date=False, parent_path=None, file_name_middle_str=CPM_FILE_NAME_MIDDLE_STR, file_name_extension=NETCDF_EXTENSION, data_type=MAX_TEMP_STR, make_paths=False)\nReturn Path of annual data files.\n\n\n&gt;&gt;&gt; str(annual_data_path(parent_path=Path('test/path')))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.annual_data_paths_generator(start_year=1980, end_year=1986, **kwargs)\nYield Path of annual data files.\n\n\n&gt;&gt;&gt; pprint(tuple(annual_data_paths_generator()))\n(PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19811201-19821130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19821201-19831130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19831201-19841130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19841201-19851130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19851201-19861130.nc'))\n&gt;&gt;&gt; parent_path_example: Iterator[Path] = annual_data_paths_generator(\n...     parent_path=Path('test/path'))\n&gt;&gt;&gt; str(next(parent_path_example))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.check_package_path(strict=True, try_chdir=False)\nReturn path for test running.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrict\nbool\nWhether to raise a ValueError if check fails.\nTrue\n\n\ntry_chdir\nbool\nWhether to attempt changing directory if initial check fails\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf strict and checks fail.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nWhether to check if call was successful.\n\n\n\n\n\n\n\n&gt;&gt;&gt; check_package_path()\nTrue\n&gt;&gt;&gt; chdir('..')\n&gt;&gt;&gt; check_package_path(strict=False)\nFalse\n&gt;&gt;&gt; check_package_path()\nTraceback (most recent call last):\n    ...\nValueError: 'clim-recal' pipeline must be run in...\n&gt;&gt;&gt; check_package_path(try_chdir=True)\nTrue\n\n\n\n\nclim_recal.utils.core.climate_data_mount_path(is_darwin=None, full_path=True)\nReturn likely climate data mount path based on operating system.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nis_darwin\nbool | None\nWhether to use CLIMATE_DATA_MOUNT_PATH_DARWIN or call is_platform_darwin if None. fixture is_platform_darwin.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe Path climate data would likely be mounted to.\n\n\n\n\n\n\n\n\nclim_recal.utils.core.csv_reader(path, **kwargs)\nYield a dict per row from a CSV file at path.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathLike\nCSV file Path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for csv.DictReader.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; csv_path: Path = TEST_AUTH_CSV_PATH\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\n&gt;&gt;&gt; tuple(csv_reader(csv_path))\n({'user_name': 'sally', 'password': 'fig*new£kid'},\n {'user_name': 'george', 'password': 'tee&iguana*sky'},\n {'user_name': 'susan', 'password': 'history!bill-walk'})\n\n\n\n\nclim_recal.utils.core.date_range_generator(start_date, end_date, inclusive=False, skip_dates=None, start_format_str=CLI_DATE_FORMAT_STR, end_format_str=CLI_DATE_FORMAT_STR, output_format_str=CLI_DATE_FORMAT_STR, skip_dates_format_str=CLI_DATE_FORMAT_STR, yield_type=date)\nReturn a tuple of DateType objects.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nDateType\nDateType at start of time series.\nrequired\n\n\nend_date\nDateType\nDateType at end of time series.\nrequired\n\n\ninclusive\nbool\nWhether to include the end_date in the returned time series.\nFalse\n\n\nskip_dates\nIterable[DateType] | DateType | None\nDates to skip between start_date and end_date.\nNone\n\n\nstart_format_str\nstr\nA strftime format to apply if start_date type is str.\nCLI_DATE_FORMAT_STR\n\n\nend_format_str\nstr\nA strftime format to apply if end_date type is str.\nCLI_DATE_FORMAT_STR\n\n\noutput_format_str\nstr\nA strftime format to apply if yield_type is str.\nCLI_DATE_FORMAT_STR\n\n\nskip_dates_format_str\nstr\nA strftime format to apply if any skip_dates are str.\nCLI_DATE_FORMAT_STR\n\n\nyield_type\ntype[date] | type[str]\nWhether which date type to return in tuple (date or str).\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIterator[DateType]\nA tuple of date or str objects (only one type throughout).\n\n\n\n\n\n\n&gt;&gt;&gt; four_years: tuple[date] = tuple(date_range_generator('19801130', '19841130'))\n&gt;&gt;&gt; len(four_years)\n1461\n&gt;&gt;&gt; four_years_inclusive: tuple[date] = tuple(\n...     date_range_generator('1980-11-30', '19841130',\n...                          inclusive=True,\n...                          start_format_str=ISO_DATE_FORMAT_STR))\n&gt;&gt;&gt; len(four_years_inclusive)\n1462\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = tuple(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates='19840229'))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1461\n&gt;&gt;&gt; skip_dates: tuple[date] = (date(1981, 12, 1), date(1982, 12, 1))\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = list(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates=skip_dates))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1460\n&gt;&gt;&gt; skip_dates in four_years_inclusive_skip\nFalse\n\n\n\n\nclim_recal.utils.core.date_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nTake start_date and end_date and return a date range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nDateType\nFirst date in range.\nrequired\n\n\nend_date\nDateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str of date range from start_date to end_date in the out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330', include_end_date=False)\n'20100101-20100329'\n\n\n\n\nclim_recal.utils.core.date_to_str(date_obj, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\nDateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str version of date_obj in out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\nclim_recal.utils.core.ensure_date(date_to_check, format_str=CLI_DATE_FORMAT_STR)\nEnsure passed date_to_check is a date.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_to_check\nDateType\nDate or str to ensure is a date.\nrequired\n\n\nformat_str\nstr\nstrptime str to convert date_to_check if necessary.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndate instance from date_to_check.\n\n\n\n\n\n\n\n&gt;&gt;&gt; ensure_date('19801130')\ndatetime.date(1980, 11, 30)\n&gt;&gt;&gt; ensure_date(date(1980, 11, 30))\ndatetime.date(1980, 11, 30)\n\n\n\n\nclim_recal.utils.core.is_climate_data_mounted(mount_path=None)\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nConsider refactoring to climate_data_mount_path returns None when conditions here return False.\n\n\n\n\nclim_recal.utils.core.is_platform_darwin()\nCheck if sys.platform is Darwin (macOS).\n\n\n\nclim_recal.utils.core.iter_to_tuple_strs(iter_var, func=str)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\nIterable[Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\nfunc\nCallable[[Any], str]\nCallable to convert each obj in iter_val to a str.\nstr\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[str, …]\nA tuple of all iter_var elements in str format.\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n&gt;&gt;&gt; iter_to_tuple_strs(\n...     ['cat', 1, Path('a/path')],\n...     lambda x: f'{x:02}' if type(x) == int else str(x))\n('cat', '01', 'a/path')\n\n\n\n\nclim_recal.utils.core.multiprocess_execute(iter, method_name=None, cpus=None)\nRun method_name as from iter via multiprocessing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter\nSequence\nSequence of instances to iterate over calling method_name from.\nrequired\n\n\nmethod_name\nstr | None\nWhat to call from objects in inter.\nNone\n\n\ncpus\nint | None\nNumber of cpus to pass to Pool for multiprocessing.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; from clim_recal.resample import CPMResampler\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     stop_index=3,\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; multiprocess_execute(cpm_resampler, method_name=\"exists\")\n[True, True, True]\n\n\n\nFailed asserting cpus &lt;= total - 1\n\n\n\n\nclim_recal.utils.core.path_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathLike\nPath to folder to iterate through.\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nGenerator[Optional[Path], None, None]\nNone if FileNotFoundError error and strict is False.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nclim_recal.utils.core.product_dict(**kwargs)\nReturn product combinatorics of kwargs.\n\n\n&gt;&gt;&gt; pprint(tuple(\n...     product_dict(animal=['cat', 'fish'], activity=('swim', 'eat'))))\n({'activity': 'swim', 'animal': 'cat'},\n {'activity': 'eat', 'animal': 'cat'},\n {'activity': 'swim', 'animal': 'fish'},\n {'activity': 'eat', 'animal': 'fish'})\n\n\n\n\nclim_recal.utils.core.range_len(maximum, start=0, stop=None, step=1)\nCacluate the total length of range with indexing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmaximum\nint\nMaximum range length.\nrequired\n\n\nstart\nint\nIndex to start from.\n0\n\n\nstop\nint | None\nIndex to stop at.\nNone\n\n\nstep\nint\nSteps between start and stop indexes\n1\n\n\n\n\n\n\n&gt;&gt;&gt; range_len(100)\n100\n&gt;&gt;&gt; range_len(100, 90)\n10\n&gt;&gt;&gt; range_len(100, 20, 30)\n10\n&gt;&gt;&gt; range_len(100, 20, 30, 2)\n5\n\n\n\n\nclim_recal.utils.core.results_path(name, path=None, time=None, extension=None, mkdir=False, dot_pre_extension=True)\nReturn Path: path/name_time.extension.\n\n\n&gt;&gt;&gt; str(results_path('hads', 'test_example/folder', extension='cat'))\n'test_example/folder/hads_..._...-....cat'\n\n\n\n\nclim_recal.utils.core.run_callable_attr(instance, method_name='execute', *args, **kwargs)\nExtract method_name from instance to call.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nobject\nobject to call method_name from.\nrequired\n\n\nmethod_name\nstr\nName of method on object to call.\n'execute'\n\n\n*args\n\nParameters passed to method_name from instance.\n()\n\n\n**kwargs\n\nParameters passed to method_name from instance.\n{}\n\n\n\n\n\n\nThis is primarily meant to address issues with pickle, particularly when using multiprocessing and lambda functions yield pickle errors.\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; run_callable_attr(jan_1, 'from_year', 1984)\ndatetime.date(1984, 1, 1)\n\n\n\n\nclim_recal.utils.core.time_str(time=None, format=RUN_TIME_STAMP_FORMAT, replace_char_tuple=None)\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\ndate | datetime | None\nTime to format. Will be set to current time if None is passed, and add current time if a date is passed to convert to a datetime.\nNone\n\n\nformat\nstr\nstrftime str format to return time as. If replace_chars is passed, these will be used to replace those in strftime.\nRUN_TIME_STAMP_FORMAT\n\n\nreplace_char_tuple\ntuple[str, str] | None\ntuple of (char_to_match, char_to_replace) order.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; time_str(datetime(2024, 10, 10, 10, 10, 10))\n'24-10-10_10-10'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html#classes",
    "href": "docs/reference/clim_recal.utils.core.html#classes",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nMonthDay\nA class to ease generating annual time series.\n\n\n\n\n\nclim_recal.utils.core.MonthDay(self, month=1, day=1, format_str=ISO_DATE_FORMAT_STR)\nA class to ease generating annual time series.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmonth\nint\nWhat Month as an integer from 1 to 12.\n\n\nday\nint\nWhat day in the month, an integer from 1 to 31.\n\n\nformat_str\nstr\nFormat to use if generating a str.\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year(1980)\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980')\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980', as_str=True)\n'1980-01-01'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_year\nReturn a date or str of date given year.\n\n\nfrom_year_range_to_str\nReturn an annual range str.\n\n\n\n\n\nclim_recal.utils.core.MonthDay.from_year(year, as_str=False)\nReturn a date or str of date given year.\n\n\n\nclim_recal.utils.core.MonthDay.from_year_range_to_str(start_year, end_year, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nReturn an annual range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nend_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nsplit_str\nstr\nstr between start_date and end_date generated.\nDATE_FORMAT_SPLIT_STR\n\n\ninclude_end_date\nbool\nWhether to include the end_date in the final str. If False follow python convention to return the day prior.\nTrue\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year_range_to_str(1980, 1982, include_end_date=False)\n'19800101-19811231'\n&gt;&gt;&gt; jan_1.from_year_range_to_str('1980', 2020)\n'19800101-20200101'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html#functions",
    "href": "docs/reference/clim_recal.utils.core.html#functions",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nannual_data_path\nReturn Path of annual data files.\n\n\nannual_data_paths_generator\nYield Path of annual data files.\n\n\ncheck_package_path\nReturn path for test running.\n\n\nclimate_data_mount_path\nReturn likely climate data mount path based on operating system.\n\n\ncsv_reader\nYield a dict per row from a CSV file at path.\n\n\ndate_range_generator\nReturn a tuple of DateType objects.\n\n\ndate_range_to_str\nTake start_date and end_date and return a date range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\nensure_date\nEnsure passed date_to_check is a date.\n\n\nis_climate_data_mounted\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nis_platform_darwin\nCheck if sys.platform is Darwin (macOS).\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmultiprocess_execute\nRun method_name as from iter via multiprocessing.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nproduct_dict\nReturn product combinatorics of kwargs.\n\n\nrange_len\nCacluate the total length of range with indexing.\n\n\nresults_path\nReturn Path: path/name_time.extension.\n\n\nrun_callable_attr\nExtract method_name from instance to call.\n\n\ntime_str\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\nclim_recal.utils.core.annual_data_path(start_year=1980, end_year=1981, month_day=DEFAULT_CPM_START_MONTH_DAY, include_end_date=False, parent_path=None, file_name_middle_str=CPM_FILE_NAME_MIDDLE_STR, file_name_extension=NETCDF_EXTENSION, data_type=MAX_TEMP_STR, make_paths=False)\nReturn Path of annual data files.\n\n\n&gt;&gt;&gt; str(annual_data_path(parent_path=Path('test/path')))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.annual_data_paths_generator(start_year=1980, end_year=1986, **kwargs)\nYield Path of annual data files.\n\n\n&gt;&gt;&gt; pprint(tuple(annual_data_paths_generator()))\n(PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19811201-19821130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19821201-19831130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19831201-19841130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19841201-19851130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19851201-19861130.nc'))\n&gt;&gt;&gt; parent_path_example: Iterator[Path] = annual_data_paths_generator(\n...     parent_path=Path('test/path'))\n&gt;&gt;&gt; str(next(parent_path_example))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.check_package_path(strict=True, try_chdir=False)\nReturn path for test running.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrict\nbool\nWhether to raise a ValueError if check fails.\nTrue\n\n\ntry_chdir\nbool\nWhether to attempt changing directory if initial check fails\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf strict and checks fail.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nWhether to check if call was successful.\n\n\n\n\n\n\n\n&gt;&gt;&gt; check_package_path()\nTrue\n&gt;&gt;&gt; chdir('..')\n&gt;&gt;&gt; check_package_path(strict=False)\nFalse\n&gt;&gt;&gt; check_package_path()\nTraceback (most recent call last):\n    ...\nValueError: 'clim-recal' pipeline must be run in...\n&gt;&gt;&gt; check_package_path(try_chdir=True)\nTrue\n\n\n\n\nclim_recal.utils.core.climate_data_mount_path(is_darwin=None, full_path=True)\nReturn likely climate data mount path based on operating system.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nis_darwin\nbool | None\nWhether to use CLIMATE_DATA_MOUNT_PATH_DARWIN or call is_platform_darwin if None. fixture is_platform_darwin.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe Path climate data would likely be mounted to.\n\n\n\n\n\n\n\n\nclim_recal.utils.core.csv_reader(path, **kwargs)\nYield a dict per row from a CSV file at path.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathLike\nCSV file Path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for csv.DictReader.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; csv_path: Path = TEST_AUTH_CSV_PATH\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\n&gt;&gt;&gt; tuple(csv_reader(csv_path))\n({'user_name': 'sally', 'password': 'fig*new£kid'},\n {'user_name': 'george', 'password': 'tee&iguana*sky'},\n {'user_name': 'susan', 'password': 'history!bill-walk'})\n\n\n\n\nclim_recal.utils.core.date_range_generator(start_date, end_date, inclusive=False, skip_dates=None, start_format_str=CLI_DATE_FORMAT_STR, end_format_str=CLI_DATE_FORMAT_STR, output_format_str=CLI_DATE_FORMAT_STR, skip_dates_format_str=CLI_DATE_FORMAT_STR, yield_type=date)\nReturn a tuple of DateType objects.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nDateType\nDateType at start of time series.\nrequired\n\n\nend_date\nDateType\nDateType at end of time series.\nrequired\n\n\ninclusive\nbool\nWhether to include the end_date in the returned time series.\nFalse\n\n\nskip_dates\nIterable[DateType] | DateType | None\nDates to skip between start_date and end_date.\nNone\n\n\nstart_format_str\nstr\nA strftime format to apply if start_date type is str.\nCLI_DATE_FORMAT_STR\n\n\nend_format_str\nstr\nA strftime format to apply if end_date type is str.\nCLI_DATE_FORMAT_STR\n\n\noutput_format_str\nstr\nA strftime format to apply if yield_type is str.\nCLI_DATE_FORMAT_STR\n\n\nskip_dates_format_str\nstr\nA strftime format to apply if any skip_dates are str.\nCLI_DATE_FORMAT_STR\n\n\nyield_type\ntype[date] | type[str]\nWhether which date type to return in tuple (date or str).\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIterator[DateType]\nA tuple of date or str objects (only one type throughout).\n\n\n\n\n\n\n&gt;&gt;&gt; four_years: tuple[date] = tuple(date_range_generator('19801130', '19841130'))\n&gt;&gt;&gt; len(four_years)\n1461\n&gt;&gt;&gt; four_years_inclusive: tuple[date] = tuple(\n...     date_range_generator('1980-11-30', '19841130',\n...                          inclusive=True,\n...                          start_format_str=ISO_DATE_FORMAT_STR))\n&gt;&gt;&gt; len(four_years_inclusive)\n1462\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = tuple(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates='19840229'))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1461\n&gt;&gt;&gt; skip_dates: tuple[date] = (date(1981, 12, 1), date(1982, 12, 1))\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = list(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates=skip_dates))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1460\n&gt;&gt;&gt; skip_dates in four_years_inclusive_skip\nFalse\n\n\n\n\nclim_recal.utils.core.date_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nTake start_date and end_date and return a date range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nDateType\nFirst date in range.\nrequired\n\n\nend_date\nDateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str of date range from start_date to end_date in the out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330', include_end_date=False)\n'20100101-20100329'\n\n\n\n\nclim_recal.utils.core.date_to_str(date_obj, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\nDateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str version of date_obj in out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\nclim_recal.utils.core.ensure_date(date_to_check, format_str=CLI_DATE_FORMAT_STR)\nEnsure passed date_to_check is a date.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_to_check\nDateType\nDate or str to ensure is a date.\nrequired\n\n\nformat_str\nstr\nstrptime str to convert date_to_check if necessary.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndate instance from date_to_check.\n\n\n\n\n\n\n\n&gt;&gt;&gt; ensure_date('19801130')\ndatetime.date(1980, 11, 30)\n&gt;&gt;&gt; ensure_date(date(1980, 11, 30))\ndatetime.date(1980, 11, 30)\n\n\n\n\nclim_recal.utils.core.is_climate_data_mounted(mount_path=None)\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nConsider refactoring to climate_data_mount_path returns None when conditions here return False.\n\n\n\n\nclim_recal.utils.core.is_platform_darwin()\nCheck if sys.platform is Darwin (macOS).\n\n\n\nclim_recal.utils.core.iter_to_tuple_strs(iter_var, func=str)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\nIterable[Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\nfunc\nCallable[[Any], str]\nCallable to convert each obj in iter_val to a str.\nstr\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[str, …]\nA tuple of all iter_var elements in str format.\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n&gt;&gt;&gt; iter_to_tuple_strs(\n...     ['cat', 1, Path('a/path')],\n...     lambda x: f'{x:02}' if type(x) == int else str(x))\n('cat', '01', 'a/path')\n\n\n\n\nclim_recal.utils.core.multiprocess_execute(iter, method_name=None, cpus=None)\nRun method_name as from iter via multiprocessing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter\nSequence\nSequence of instances to iterate over calling method_name from.\nrequired\n\n\nmethod_name\nstr | None\nWhat to call from objects in inter.\nNone\n\n\ncpus\nint | None\nNumber of cpus to pass to Pool for multiprocessing.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; from clim_recal.resample import CPMResampler\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     stop_index=3,\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; multiprocess_execute(cpm_resampler, method_name=\"exists\")\n[True, True, True]\n\n\n\nFailed asserting cpus &lt;= total - 1\n\n\n\n\nclim_recal.utils.core.path_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nPathLike\nPath to folder to iterate through.\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nGenerator[Optional[Path], None, None]\nNone if FileNotFoundError error and strict is False.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nclim_recal.utils.core.product_dict(**kwargs)\nReturn product combinatorics of kwargs.\n\n\n&gt;&gt;&gt; pprint(tuple(\n...     product_dict(animal=['cat', 'fish'], activity=('swim', 'eat'))))\n({'activity': 'swim', 'animal': 'cat'},\n {'activity': 'eat', 'animal': 'cat'},\n {'activity': 'swim', 'animal': 'fish'},\n {'activity': 'eat', 'animal': 'fish'})\n\n\n\n\nclim_recal.utils.core.range_len(maximum, start=0, stop=None, step=1)\nCacluate the total length of range with indexing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmaximum\nint\nMaximum range length.\nrequired\n\n\nstart\nint\nIndex to start from.\n0\n\n\nstop\nint | None\nIndex to stop at.\nNone\n\n\nstep\nint\nSteps between start and stop indexes\n1\n\n\n\n\n\n\n&gt;&gt;&gt; range_len(100)\n100\n&gt;&gt;&gt; range_len(100, 90)\n10\n&gt;&gt;&gt; range_len(100, 20, 30)\n10\n&gt;&gt;&gt; range_len(100, 20, 30, 2)\n5\n\n\n\n\nclim_recal.utils.core.results_path(name, path=None, time=None, extension=None, mkdir=False, dot_pre_extension=True)\nReturn Path: path/name_time.extension.\n\n\n&gt;&gt;&gt; str(results_path('hads', 'test_example/folder', extension='cat'))\n'test_example/folder/hads_..._...-....cat'\n\n\n\n\nclim_recal.utils.core.run_callable_attr(instance, method_name='execute', *args, **kwargs)\nExtract method_name from instance to call.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nobject\nobject to call method_name from.\nrequired\n\n\nmethod_name\nstr\nName of method on object to call.\n'execute'\n\n\n*args\n\nParameters passed to method_name from instance.\n()\n\n\n**kwargs\n\nParameters passed to method_name from instance.\n{}\n\n\n\n\n\n\nThis is primarily meant to address issues with pickle, particularly when using multiprocessing and lambda functions yield pickle errors.\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; run_callable_attr(jan_1, 'from_year', 1984)\ndatetime.date(1984, 1, 1)\n\n\n\n\nclim_recal.utils.core.time_str(time=None, format=RUN_TIME_STAMP_FORMAT, replace_char_tuple=None)\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\ndate | datetime | None\nTime to format. Will be set to current time if None is passed, and add current time if a date is passed to convert to a datetime.\nNone\n\n\nformat\nstr\nstrftime str format to return time as. If replace_chars is passed, these will be used to replace those in strftime.\nRUN_TIME_STAMP_FORMAT\n\n\nreplace_char_tuple\ntuple[str, str] | None\ntuple of (char_to_match, char_to_replace) order.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; time_str(datetime(2024, 10, 10, 10, 10, 10))\n'24-10-10_10-10'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html",
    "href": "docs/reference/clim_recal.utils.xarray.html",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "clim_recal.utils.xarray\n\n\n\n\n\nName\nDescription\n\n\n\n\nXarrayTimeSeriesCalcManager\nManage cacluations over time for .nc files.\n\n\n\n\n\nclim_recal.utils.xarray.XarrayTimeSeriesCalcManager(self, path=climate_data_mount_path() / 'Raw/UKCP2.2/', save_folder=Path('../docs/assets/cpm-raw-medians'), sub_path=Path('latest'), variables=VariableOptions.cpm_values(), runs=RunOptions.preferred_and_first(), method_name='median', time_dim_name='time', regex=CPM_REGEX, source_folders=list())\nManage cacluations over time for .nc files.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nPathLike\nPath to aggreate raw files from, following standard cpm hierarchy.\n\n\nsub_path\nPathLike | None\nA subpath to parse, like ‘latest’ for UKCPM2.2.\n\n\nsave_folder\nPathLike\nWhere to save resulting summary nc files.\n\n\nvariables\nSequence[str | VariableOptions]\nWhich variables to include\n\n\nruns\nSequence[str | RunOptions]\nWhich RunOptions to include.\n\n\nmethod_name\nstr\nWhich method to use to aggreate. Must be a standard xarray Dataset method.\n\n\ntime_dim_name\nstr\nName of the temporal dim on the joined .nc files.\n\n\nregex\nstr\nCheck .nc paths to match and then aggregate.\n\n\nsource_folders\nlist\nList of folders to iterate over, filled via path if None.\n\n\n\n\n\n\n&gt;&gt;&gt; tasmax_hads_1980_raw = getfixture('tasmax_hads_1980_raw')\n&gt;&gt;&gt; if not tasmax_hads_1980_raw:\n...     pytest.skip(mount_or_cache_doctest_skip_message)\n&gt;&gt;&gt; tmp_save: Path = getfixture('tmp_path') / 'xarray-time-series-summary-manager'\n&gt;&gt;&gt; xr_var_managers = XarrayTimeSeriesCalcManager(save_folder=tmp_save)\n&gt;&gt;&gt; save_paths: tuple[Path, ...] = xr_var_managers.save_joined_xr_time_series(stop=2, ts_stop=2)\nAggregating '05' 'tasmax' (1/2)...\nAggregating '06' 'tasmax' (2/2)...\n&gt;&gt;&gt; pprint(save_paths)\n(...Path('.../median-tasmax-05.nc'),\n ...Path('.../median-tasmax-06.nc'))\n&gt;&gt;&gt; pprint(sorted(tmp_save.iterdir()))\n[...Path('.../median-tasmax-05.nc'),\n ...Path('.../median-tasmax-06.nc')]\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nannual_group_xr_time_series\nReturn and plot a Dataset of time series temporally overlayed.\n\n\napply_geo_func\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\ncftime360_to_date\nConvert a Datetime360Day into a date.\n\n\ncftime_range_gen\nConvert a banded time index a banded standard (Gregorian).\n\n\ncheck_xarray_path_and_var_name\nCheck and return a T_Dataset instances and included variable name.\n\n\nconvert_xr_calendar\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\ncpm_check_converted\nCheck if cpm_xr_time_series is likely already reprojected.\n\n\ncpm_reproject_with_standard_calendar\nConvert raw cpm_xr_time_series to an 365/366 days and 27700 coords.\n\n\ncpm_xarray_to_standard_calendar\nConvert a CPM nc file of 360 day calendars to standard calendar.\n\n\ncrop_xarray\nCrop xr_time_series with crop_path shapefile.\n\n\nensure_xr_dataset\nReturn xr_time_series as a xarray.Dataset instance.\n\n\nfile_name_to_start_end_dates\nReturn dates of file name with date_format-date_format structure.\n\n\ngdal_warp_wrapper\nExecute the gdalwrap function within python.\n\n\ngenerate_360_to_standard\nReturn array_to_expand 360 days expanded to 365 or 366 days.\n\n\nget_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\nhads_resample_and_reproject\nResample HADs xarray time series to 2.2km.\n\n\ninterpolate_xr_ts_nans\nInterpolate nan values in a Dataset time series.\n\n\njoin_xr_time_series_var\nJoin a set of xr_time_series files chronologically.\n\n\nplot_xarray\nPlot da with **kwargs to path.\n\n\nregion_crop_file_name\nGenerate a file name for a regional crop.\n\n\nxr_reproject_crs\nReproject source_xr to target_xr coordinate structure.\n\n\n\n\n\nclim_recal.utils.xarray.annual_group_xr_time_series(joined_xr_time_series, variable_name, groupby_method='time.dayofyear', method_name='median', time_dim_name='time', regex=CPM_REGEX, start=0, stop=None, step=1, plot_path='annual-aggregated.png', time_stamp=True, **kwargs)\nReturn and plot a Dataset of time series temporally overlayed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njoined_xr_time_series\nT_Dataset | PathLike\nProvide existing Dataset to aggregate and plot (otherwise check path).\nrequired\n\n\nvariable_name\nstr\nA variable name to specify for data expected. If none that will be extracted and checked from the files directly.\nrequired\n\n\ngroupby_method\nstr\nxarray method to aggregate time of xr_time_series.\n'time.dayofyear'\n\n\nmethod_name\nstr\nxarray method to calculate for plot.\n'median'\n\n\ntime_dim_name\nstr\nName of time dimension in passed files.\n'time'\n\n\nregex\nstr\nA str to filter files within path\nCPM_REGEX\n\n\nstart\nint\nWhat point to start indexing path results from.\n0\n\n\nstop\nint | None\nWhat point to stop indexing path restuls from.\nNone\n\n\nstep\nint\nHow many paths to jump between when iterating between stop and start.\n1\n\n\nplot_path\nPathLike\nPath to save plot to.\n'annual-aggregated.png'\n\n\ntime_stamp\nbool\nWhether to include a time stamp in the plot_path name.\nTrue\n\n\n**kwargs\n\nAdditional parameters to pass to plot_xarray.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; tasmax_cpm_1980_raw = getfixture('tasmax_cpm_1980_raw')\n&gt;&gt;&gt; if not tasmax_cpm_1980_raw:\n...     pytest.skip(mount_or_cache_doctest_skip_message)\n&gt;&gt;&gt; tasmax_cpm_1980_raw_path = getfixture('tasmax_cpm_1980_raw_path').parents[1]\n&gt;&gt;&gt; results: T_Dataset = annual_group_xr_time_series(\n...     tasmax_cpm_1980_raw_path, 'tasmax', stop=3)\n&gt;&gt;&gt; results\n&lt;xarray.Dataset&gt; ...\nDimensions:    (dayofyear: 360)\nCoordinates:\n  * dayofyear  (dayofyear) int64 ... 1 2 3 4 5 6 7 ... 355 356 357 358 359 360\nData variables:\n    tasmax     (dayofyear) float64 ... 9.2 8.95 8.408 8.747 ... 6.387 8.15 9.132\n\n\n\n\nclim_recal.utils.xarray.apply_geo_func(source_path, func, export_folder, new_path_name_func=None, to_netcdf=True, to_raster=False, export_path_as_output_path_kwarg=False, return_results=False, **kwargs)\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_path\nPathLike\nnetcdf file to apply func to.\nrequired\n\n\nfunc\nReprojectFuncType\nCallable to modify netcdf.\nrequired\n\n\nexport_folder\nPathLike\nWhere to save results.\nrequired\n\n\nnew_path_name_func\nCallable[[Path], Path] | None\nCallabe to generate new path to save to.\nNone\n\n\nto_netcdf\nbool\nWhether to call to_netcdf() method on results Dataset.\nTrue\n\n\nto_raster\nbool\nWhether to call rio.to_raster() on results Dataset.\nFalse\n\n\nexport_path_as_output_path_kwarg\nbool\nWhether to add output_path = export_path to kwargs passed to func. Meant for cases calling gdal_warp_wrapper.\nFalse\n\n\nreturn_results\nbool\nWhether to return results, which would be a Dataset or GDALDataset (the latter if gdal_warp_wrapper is used).\nFalse\n\n\n**kwargs\n\nOther parameters passed to func call.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nEither a Path to generated file or converted xarray object.\n\n\n\n\n\n\n\n\nclim_recal.utils.xarray.cftime360_to_date(cf_360)\nConvert a Datetime360Day into a date.\n\n\n&gt;&gt;&gt; cftime360_to_date(Datetime360Day(1980, 1, 1))\ndatetime.date(1980, 1, 1)\n\n\n\n\nclim_recal.utils.xarray.cftime_range_gen(time_data_array, **kwargs)\nConvert a banded time index a banded standard (Gregorian).\n\n\n\nclim_recal.utils.xarray.check_xarray_path_and_var_name(xr_time_series, variable_name, ignore_warnings=True)\nCheck and return a T_Dataset instances and included variable name.\n\n\n\nclim_recal.utils.xarray.convert_xr_calendar(xr_time_series, align_on=DEFAULT_CALENDAR_ALIGN, calendar=CFCalendarSTANDARD, use_cftime=False, missing_value=np.nan, interpolate_na=False, ensure_output_type_is_dataset=False, interpolate_method=DEFAULT_INTERPOLATION_METHOD, keep_crs=True, keep_attrs=True, limit=1, engine=NETCDF4_XARRAY_ENGINE, check_cftime_cols=None, cftime_range_gen_kwargs=None)\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\nShort time examples (like 2 skipped out of 8 days) raises: ValueError(\"date_range_like was unable to generate a range as the source frequency was not inferable.\")\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxr_time_series\nT_DataArray | T_Dataset | PathLike\nA DataArray or Dataset to convert to calendar time.\nrequired\n\n\nalign_on\nConvertCalendarAlignOptions\nWhether and how to align calendar types.\nDEFAULT_CALENDAR_ALIGN\n\n\ncalendar\nCFCalendar\nType of calendar to convert xr_time_series to.\nCFCalendarSTANDARD\n\n\nuse_cftime\nbool\nWhether to enforce cftime vs datetime64 time format.\nFalse\n\n\nmissing_value\nAny | None\nMissing value to populate missing date interpolations with.\nnp.nan\n\n\nkeep_crs\nbool\nReapply initial Coordinate Reference System (CRS) after time projection.\nTrue\n\n\ninterpolate_na\nbool\nWhether to apply temporal interpolation for missing values.\nFalse\n\n\ninterpolate_method\nInterpOptions\nWhich InterpOptions method to apply if interpolate_na is True.\nDEFAULT_INTERPOLATION_METHOD\n\n\nkeep_attrs\nbool\nWhether to keep all attributes on after interpolate_na\nTrue\n\n\nlimit\nint\nLimit of number of continuous missing day values allowed in interpolate_na.\n1\n\n\nengine\nXArrayEngineType\nWhich XArrayEngineType to use in parsing files and operations.\nNETCDF4_XARRAY_ENGINE\n\n\nextrapolate_fill_value\n\nIf True, then pass fill_value=extrapolate. See: * https://docs.xarray.dev/en/stable/generated/xarray.Dataset.interpolate_na.html * https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d\nrequired\n\n\ncheck_cftime_cols\ntuple[str] | None\nColumns to check cftime format on\nNone\n\n\ncftime_range_gen_kwargs\ndict[str, Any] | None\nAny kwargs to pass to cftime_range_gen\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nLikely from xarray calling date_range_like.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nT_DataArrayOrSet\nConverted xr_time_series to specified calendar with optional interpolation.\n\n\n\n\n\n\nCertain values may fail to interpolate in cases of 360 -&gt; 365/366 (Gregorian) calendar. Examples include projecting CPM data, which is able to fill in measurement values (e.g. tasmax) but the year and month_number variables have nan values",
    "crumbs": [
      "API Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html#classes",
    "href": "docs/reference/clim_recal.utils.xarray.html#classes",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nXarrayTimeSeriesCalcManager\nManage cacluations over time for .nc files.\n\n\n\n\n\nclim_recal.utils.xarray.XarrayTimeSeriesCalcManager(self, path=climate_data_mount_path() / 'Raw/UKCP2.2/', save_folder=Path('../docs/assets/cpm-raw-medians'), sub_path=Path('latest'), variables=VariableOptions.cpm_values(), runs=RunOptions.preferred_and_first(), method_name='median', time_dim_name='time', regex=CPM_REGEX, source_folders=list())\nManage cacluations over time for .nc files.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nPathLike\nPath to aggreate raw files from, following standard cpm hierarchy.\n\n\nsub_path\nPathLike | None\nA subpath to parse, like ‘latest’ for UKCPM2.2.\n\n\nsave_folder\nPathLike\nWhere to save resulting summary nc files.\n\n\nvariables\nSequence[str | VariableOptions]\nWhich variables to include\n\n\nruns\nSequence[str | RunOptions]\nWhich RunOptions to include.\n\n\nmethod_name\nstr\nWhich method to use to aggreate. Must be a standard xarray Dataset method.\n\n\ntime_dim_name\nstr\nName of the temporal dim on the joined .nc files.\n\n\nregex\nstr\nCheck .nc paths to match and then aggregate.\n\n\nsource_folders\nlist\nList of folders to iterate over, filled via path if None.\n\n\n\n\n\n\n&gt;&gt;&gt; tasmax_hads_1980_raw = getfixture('tasmax_hads_1980_raw')\n&gt;&gt;&gt; if not tasmax_hads_1980_raw:\n...     pytest.skip(mount_or_cache_doctest_skip_message)\n&gt;&gt;&gt; tmp_save: Path = getfixture('tmp_path') / 'xarray-time-series-summary-manager'\n&gt;&gt;&gt; xr_var_managers = XarrayTimeSeriesCalcManager(save_folder=tmp_save)\n&gt;&gt;&gt; save_paths: tuple[Path, ...] = xr_var_managers.save_joined_xr_time_series(stop=2, ts_stop=2)\nAggregating '05' 'tasmax' (1/2)...\nAggregating '06' 'tasmax' (2/2)...\n&gt;&gt;&gt; pprint(save_paths)\n(...Path('.../median-tasmax-05.nc'),\n ...Path('.../median-tasmax-06.nc'))\n&gt;&gt;&gt; pprint(sorted(tmp_save.iterdir()))\n[...Path('.../median-tasmax-05.nc'),\n ...Path('.../median-tasmax-06.nc')]",
    "crumbs": [
      "API Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html#functions",
    "href": "docs/reference/clim_recal.utils.xarray.html#functions",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nannual_group_xr_time_series\nReturn and plot a Dataset of time series temporally overlayed.\n\n\napply_geo_func\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\ncftime360_to_date\nConvert a Datetime360Day into a date.\n\n\ncftime_range_gen\nConvert a banded time index a banded standard (Gregorian).\n\n\ncheck_xarray_path_and_var_name\nCheck and return a T_Dataset instances and included variable name.\n\n\nconvert_xr_calendar\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\ncpm_check_converted\nCheck if cpm_xr_time_series is likely already reprojected.\n\n\ncpm_reproject_with_standard_calendar\nConvert raw cpm_xr_time_series to an 365/366 days and 27700 coords.\n\n\ncpm_xarray_to_standard_calendar\nConvert a CPM nc file of 360 day calendars to standard calendar.\n\n\ncrop_xarray\nCrop xr_time_series with crop_path shapefile.\n\n\nensure_xr_dataset\nReturn xr_time_series as a xarray.Dataset instance.\n\n\nfile_name_to_start_end_dates\nReturn dates of file name with date_format-date_format structure.\n\n\ngdal_warp_wrapper\nExecute the gdalwrap function within python.\n\n\ngenerate_360_to_standard\nReturn array_to_expand 360 days expanded to 365 or 366 days.\n\n\nget_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\nhads_resample_and_reproject\nResample HADs xarray time series to 2.2km.\n\n\ninterpolate_xr_ts_nans\nInterpolate nan values in a Dataset time series.\n\n\njoin_xr_time_series_var\nJoin a set of xr_time_series files chronologically.\n\n\nplot_xarray\nPlot da with **kwargs to path.\n\n\nregion_crop_file_name\nGenerate a file name for a regional crop.\n\n\nxr_reproject_crs\nReproject source_xr to target_xr coordinate structure.\n\n\n\n\n\nclim_recal.utils.xarray.annual_group_xr_time_series(joined_xr_time_series, variable_name, groupby_method='time.dayofyear', method_name='median', time_dim_name='time', regex=CPM_REGEX, start=0, stop=None, step=1, plot_path='annual-aggregated.png', time_stamp=True, **kwargs)\nReturn and plot a Dataset of time series temporally overlayed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\njoined_xr_time_series\nT_Dataset | PathLike\nProvide existing Dataset to aggregate and plot (otherwise check path).\nrequired\n\n\nvariable_name\nstr\nA variable name to specify for data expected. If none that will be extracted and checked from the files directly.\nrequired\n\n\ngroupby_method\nstr\nxarray method to aggregate time of xr_time_series.\n'time.dayofyear'\n\n\nmethod_name\nstr\nxarray method to calculate for plot.\n'median'\n\n\ntime_dim_name\nstr\nName of time dimension in passed files.\n'time'\n\n\nregex\nstr\nA str to filter files within path\nCPM_REGEX\n\n\nstart\nint\nWhat point to start indexing path results from.\n0\n\n\nstop\nint | None\nWhat point to stop indexing path restuls from.\nNone\n\n\nstep\nint\nHow many paths to jump between when iterating between stop and start.\n1\n\n\nplot_path\nPathLike\nPath to save plot to.\n'annual-aggregated.png'\n\n\ntime_stamp\nbool\nWhether to include a time stamp in the plot_path name.\nTrue\n\n\n**kwargs\n\nAdditional parameters to pass to plot_xarray.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; tasmax_cpm_1980_raw = getfixture('tasmax_cpm_1980_raw')\n&gt;&gt;&gt; if not tasmax_cpm_1980_raw:\n...     pytest.skip(mount_or_cache_doctest_skip_message)\n&gt;&gt;&gt; tasmax_cpm_1980_raw_path = getfixture('tasmax_cpm_1980_raw_path').parents[1]\n&gt;&gt;&gt; results: T_Dataset = annual_group_xr_time_series(\n...     tasmax_cpm_1980_raw_path, 'tasmax', stop=3)\n&gt;&gt;&gt; results\n&lt;xarray.Dataset&gt; ...\nDimensions:    (dayofyear: 360)\nCoordinates:\n  * dayofyear  (dayofyear) int64 ... 1 2 3 4 5 6 7 ... 355 356 357 358 359 360\nData variables:\n    tasmax     (dayofyear) float64 ... 9.2 8.95 8.408 8.747 ... 6.387 8.15 9.132\n\n\n\n\nclim_recal.utils.xarray.apply_geo_func(source_path, func, export_folder, new_path_name_func=None, to_netcdf=True, to_raster=False, export_path_as_output_path_kwarg=False, return_results=False, **kwargs)\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_path\nPathLike\nnetcdf file to apply func to.\nrequired\n\n\nfunc\nReprojectFuncType\nCallable to modify netcdf.\nrequired\n\n\nexport_folder\nPathLike\nWhere to save results.\nrequired\n\n\nnew_path_name_func\nCallable[[Path], Path] | None\nCallabe to generate new path to save to.\nNone\n\n\nto_netcdf\nbool\nWhether to call to_netcdf() method on results Dataset.\nTrue\n\n\nto_raster\nbool\nWhether to call rio.to_raster() on results Dataset.\nFalse\n\n\nexport_path_as_output_path_kwarg\nbool\nWhether to add output_path = export_path to kwargs passed to func. Meant for cases calling gdal_warp_wrapper.\nFalse\n\n\nreturn_results\nbool\nWhether to return results, which would be a Dataset or GDALDataset (the latter if gdal_warp_wrapper is used).\nFalse\n\n\n**kwargs\n\nOther parameters passed to func call.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nEither a Path to generated file or converted xarray object.\n\n\n\n\n\n\n\n\nclim_recal.utils.xarray.cftime360_to_date(cf_360)\nConvert a Datetime360Day into a date.\n\n\n&gt;&gt;&gt; cftime360_to_date(Datetime360Day(1980, 1, 1))\ndatetime.date(1980, 1, 1)\n\n\n\n\nclim_recal.utils.xarray.cftime_range_gen(time_data_array, **kwargs)\nConvert a banded time index a banded standard (Gregorian).\n\n\n\nclim_recal.utils.xarray.check_xarray_path_and_var_name(xr_time_series, variable_name, ignore_warnings=True)\nCheck and return a T_Dataset instances and included variable name.\n\n\n\nclim_recal.utils.xarray.convert_xr_calendar(xr_time_series, align_on=DEFAULT_CALENDAR_ALIGN, calendar=CFCalendarSTANDARD, use_cftime=False, missing_value=np.nan, interpolate_na=False, ensure_output_type_is_dataset=False, interpolate_method=DEFAULT_INTERPOLATION_METHOD, keep_crs=True, keep_attrs=True, limit=1, engine=NETCDF4_XARRAY_ENGINE, check_cftime_cols=None, cftime_range_gen_kwargs=None)\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\nShort time examples (like 2 skipped out of 8 days) raises: ValueError(\"date_range_like was unable to generate a range as the source frequency was not inferable.\")\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxr_time_series\nT_DataArray | T_Dataset | PathLike\nA DataArray or Dataset to convert to calendar time.\nrequired\n\n\nalign_on\nConvertCalendarAlignOptions\nWhether and how to align calendar types.\nDEFAULT_CALENDAR_ALIGN\n\n\ncalendar\nCFCalendar\nType of calendar to convert xr_time_series to.\nCFCalendarSTANDARD\n\n\nuse_cftime\nbool\nWhether to enforce cftime vs datetime64 time format.\nFalse\n\n\nmissing_value\nAny | None\nMissing value to populate missing date interpolations with.\nnp.nan\n\n\nkeep_crs\nbool\nReapply initial Coordinate Reference System (CRS) after time projection.\nTrue\n\n\ninterpolate_na\nbool\nWhether to apply temporal interpolation for missing values.\nFalse\n\n\ninterpolate_method\nInterpOptions\nWhich InterpOptions method to apply if interpolate_na is True.\nDEFAULT_INTERPOLATION_METHOD\n\n\nkeep_attrs\nbool\nWhether to keep all attributes on after interpolate_na\nTrue\n\n\nlimit\nint\nLimit of number of continuous missing day values allowed in interpolate_na.\n1\n\n\nengine\nXArrayEngineType\nWhich XArrayEngineType to use in parsing files and operations.\nNETCDF4_XARRAY_ENGINE\n\n\nextrapolate_fill_value\n\nIf True, then pass fill_value=extrapolate. See: * https://docs.xarray.dev/en/stable/generated/xarray.Dataset.interpolate_na.html * https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d\nrequired\n\n\ncheck_cftime_cols\ntuple[str] | None\nColumns to check cftime format on\nNone\n\n\ncftime_range_gen_kwargs\ndict[str, Any] | None\nAny kwargs to pass to cftime_range_gen\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nLikely from xarray calling date_range_like.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nT_DataArrayOrSet\nConverted xr_time_series to specified calendar with optional interpolation.\n\n\n\n\n\n\nCertain values may fail to interpolate in cases of 360 -&gt; 365/366 (Gregorian) calendar. Examples include projecting CPM data, which is able to fill in measurement values (e.g. tasmax) but the year and month_number variables have nan values",
    "crumbs": [
      "API Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html",
    "href": "docs/reference/clim_recal.utils.server.html",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "clim_recal.utils.server\nUtility functions.\n\n\n\n\n\nName\nDescription\n\n\n\n\nCondaLockFileManager\nRun conda_lock install to generate conda yml.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager(self, conda_file_path=DEFAULT_CONDA_LOCK_PATH, env_paths=DEFAULT_ENV_PATHS, replace_file_path=False, legacy_arch=GITHUB_ACTIONS_ARCHITECTURE, legacy_name_prefix=CONDA_LEGACY_PREFIX, default_kwargs=lambda: DEFAULT_CONDA_LOCK_KWARGS())\nRun conda_lock install to generate conda yml.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nconda_file_path\nPathLike\nPath to write conda-lock file to.\n\n\nenv_paths\nSequence[PathLike]\nPaths of configs to combine. For supported formats see: https://conda.github.io/conda-lock/\n\n\nreplace_file_path\nbool\nWhether to replace file_path if it already exists.\n\n\nlegacy_arch\nstr | None\nWhat archeticture to use for legacy export.\n\n\nlegacy_name_prefix\nPathLike | str\nstr to precede legacy_arch export file if run_legacy_mv() is run.\n\n\ndefault_kwargs\ndict[str, Any]\nkwargs to pass to self.run_conda_lock().\n\n\n\n\n\n\nThis is derived from automating, with the -p osx-64 etc. components now specified in pyproject.toml and environment.yml, the following command:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64\nA full exmaple with options matching saved defaults:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64 --check-input-hash\n\n\n\n&gt;&gt;&gt; conda_lock = CondaLockFileManager()\n&gt;&gt;&gt; conda_lock\n&lt;CondaLockFileManager(conda_file_path='../conda-lock.yml', env_paths=('../environment.yml', 'pyproject.toml'), legacy_arch='linux-64')&gt;\n&gt;&gt;&gt; conda_lock.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; conda_lock.run(as_str=True, use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconda_lock_cmd_str\nReturn configured conda-lock command.\n\n\nlegacy_export_cmd_str\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\nrun\nReturn self configurations, optionally execute as subprocess.\n\n\nrun_conda_lock\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\nrun_legacy_export\nRun self.legacy_export_cmd_str().\n\n\nrun_legacy_mv\nRun self.legacy_export_cmd_str.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.conda_lock_cmd_str(use_default_kwargs=False, **kwargs)\nReturn configured conda-lock command.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.legacy_export_cmd_str(**kwargs)\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run(as_str=False, include_all=False, execute_all=False, conda_lock=True, execute_conda_lock=False, use_default_kwargs=False, legacy_export=False, execute_legacy_export=False, legacy_move=False, execute_legacy_move=False, cmds_list=None, execute_priors=False, cmds_post_list=None, execute_cmds_post=False, parent_dir_after_lock=False, **kwargs)\nReturn self configurations, optionally execute as subprocess.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nas_str\nbool\nWhether to return as a str, if not as a list[str].\nFalse\n\n\ninclude_all\nbool\nInclude all commands, overriding individual parameters like conda_lock etc. Combine with execute_all to also run.\nFalse\n\n\nexecute_all\nbool\nRun all included commands, overriding individual parameters like execute_conda_lock etc. Combine with include_all to run all commands.\nFalse\n\n\nconda_lock\nbool\nWhether to include self.run_conda_lock().\nTrue\n\n\nexecute_conda_lock\nbool\nWhether to run the generated commands via subprocess.run().\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to use self.default_kwargs params to run self.run_conda_lock().\nFalse\n\n\nlegacy_export\nbool\nWhether to add the self.legacy_export_cmd_str command.\nFalse\n\n\nexecute_legacy_export\nbool\nWhether to run the self.legacy_export_cmd_str().\nFalse\n\n\nlegacy_move\nbool\nWhether to add the self.legacy_mv_cmd_str() command.\nFalse\n\n\nexecute_legacy_move\nbool\nWhether to run the self.legacy_mv_cmd_str().\nFalse\n\n\ncmds_list\nlist[str] | None\nA list of commands to execute. If passed, these are executed prior.\nNone\n\n\nexecute_priors\nbool\nExecute commands passed in cmds_list prior to any others.\nFalse\n\n\ncmds_post_list\nlist[str] | None\nA list of commands to run after all others.\nNone\n\n\nexecute_cmds_post\nbool\nExecute commands passed in cmds_post_list after all others.\nFalse\n\n\nparent_dir_after_lock\nbool\nWhether to return to parent dir after lock command.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str] | str\nA list of commands generated, or a str of each command separated by a newline character (\\n).\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; print(conda_lock_file_manager.run(\n...     as_str=True, legacy_export=True, legacy_move=True))\nconda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml\nconda-lock render --kind explicit --platform linux-64\nmv conda-linux-64.lock .conda-linux-64.lock\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_conda_lock(execute=False, use_default_kwargs=False, parent_dir_after_lock=False, **kwargs)\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to include the self.default_kwargs in run.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.conda_lock_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock()\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(pdb=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_export(execute=False, **kwargs)\nRun self.legacy_export_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.legacy_export_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export()\n'conda-lock render --kind explicit --platform linux-64'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export(pdb=True)\n'conda-lock render --kind explicit --platform linux-64 --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_mv(execute=False)\nRun self.legacy_export_cmd_str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock .conda-linux-64.lock'\n&gt;&gt;&gt; conda_lock_file_manager.legacy_name_prefix = '../.'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock ../.conda-linux-64.lock'\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nkwargs_to_cli_str\nConvert kwargs into a cli str.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\nrm_user\nRemove user and user home folder.\n\n\nset_and_pop_attr_kwargs\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\nclim_recal.utils.server.kwargs_to_cli_str(space_prefix=True, **kwargs)\nConvert kwargs into a cli str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nkey=val parameters to concatenate as str.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA final str of concatenated **kwargs in command line form.\n\n\n\n\n\n\n&gt;&gt;&gt; kwargs_to_cli_str(cat=4, in_a=\"hat\", fun=False)\n' --cat 4 --in-a hat --not-fun'\n&gt;&gt;&gt; kwargs_to_cli_str(space_prefix=False, cat=4, fun=True)\n'--cat 4 --fun'\n&gt;&gt;&gt; kwargs_to_cli_str()\n''\n\n\n\n\nclim_recal.utils.server.make_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nName for user and home folder name to append to user_home_path.\nrequired\n\n\npassword\nstr\nLogin password.\nrequired\n\n\ncode_path\nPathLike\nPath to copy code from to user home directory.\nRSTUDIO_DOCKER_USER_PATH\n\n\nuser_home_path\nPathLike\nPath that user folder will be in, often Path('/home') in linux.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPath\nFull path to generated user home folder.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'an_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = JUPYTER_DOCKER_USER_PATH\n&gt;&gt;&gt; make_user(user_name, password, code_path=code_path)\nPosixPath('/home/an_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'an_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.make_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\nPathLike\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\nCallable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; tmp_data_path = getfixture('data_fixtures_path')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = tmp_data_path / 'test_user_accounts.xlsx'\n&gt;&gt;&gt; assert test_accounts_path.exists()\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file()\n...  for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\nclim_recal.utils.server.rm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\nPathLike\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nuser name of account and home folder deleted.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.set_and_pop_attr_kwargs(instance, **kwargs)\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nAny\nAn object to modify.\nrequired\n\n\nkwargs\n\nkey: val parameters to potentially modify instance attributes.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, Any]\nAny remaining kwargs not used to modify instance.\n\n\n\n\n\n\n&gt;&gt;&gt; kwrgs = set_and_pop_attr_kwargs(\n...    conda_lock_file_manager, env_paths=['pyproject.toml'], cat=4)\n&gt;&gt;&gt; conda_lock_file_manager.env_paths\n['pyproject.toml']\n&gt;&gt;&gt; kwrgs\n{'cat': 4}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html#classes",
    "href": "docs/reference/clim_recal.utils.server.html#classes",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCondaLockFileManager\nRun conda_lock install to generate conda yml.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager(self, conda_file_path=DEFAULT_CONDA_LOCK_PATH, env_paths=DEFAULT_ENV_PATHS, replace_file_path=False, legacy_arch=GITHUB_ACTIONS_ARCHITECTURE, legacy_name_prefix=CONDA_LEGACY_PREFIX, default_kwargs=lambda: DEFAULT_CONDA_LOCK_KWARGS())\nRun conda_lock install to generate conda yml.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nconda_file_path\nPathLike\nPath to write conda-lock file to.\n\n\nenv_paths\nSequence[PathLike]\nPaths of configs to combine. For supported formats see: https://conda.github.io/conda-lock/\n\n\nreplace_file_path\nbool\nWhether to replace file_path if it already exists.\n\n\nlegacy_arch\nstr | None\nWhat archeticture to use for legacy export.\n\n\nlegacy_name_prefix\nPathLike | str\nstr to precede legacy_arch export file if run_legacy_mv() is run.\n\n\ndefault_kwargs\ndict[str, Any]\nkwargs to pass to self.run_conda_lock().\n\n\n\n\n\n\nThis is derived from automating, with the -p osx-64 etc. components now specified in pyproject.toml and environment.yml, the following command:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64\nA full exmaple with options matching saved defaults:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64 --check-input-hash\n\n\n\n&gt;&gt;&gt; conda_lock = CondaLockFileManager()\n&gt;&gt;&gt; conda_lock\n&lt;CondaLockFileManager(conda_file_path='../conda-lock.yml', env_paths=('../environment.yml', 'pyproject.toml'), legacy_arch='linux-64')&gt;\n&gt;&gt;&gt; conda_lock.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; conda_lock.run(as_str=True, use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconda_lock_cmd_str\nReturn configured conda-lock command.\n\n\nlegacy_export_cmd_str\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\nrun\nReturn self configurations, optionally execute as subprocess.\n\n\nrun_conda_lock\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\nrun_legacy_export\nRun self.legacy_export_cmd_str().\n\n\nrun_legacy_mv\nRun self.legacy_export_cmd_str.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.conda_lock_cmd_str(use_default_kwargs=False, **kwargs)\nReturn configured conda-lock command.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.legacy_export_cmd_str(**kwargs)\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run(as_str=False, include_all=False, execute_all=False, conda_lock=True, execute_conda_lock=False, use_default_kwargs=False, legacy_export=False, execute_legacy_export=False, legacy_move=False, execute_legacy_move=False, cmds_list=None, execute_priors=False, cmds_post_list=None, execute_cmds_post=False, parent_dir_after_lock=False, **kwargs)\nReturn self configurations, optionally execute as subprocess.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nas_str\nbool\nWhether to return as a str, if not as a list[str].\nFalse\n\n\ninclude_all\nbool\nInclude all commands, overriding individual parameters like conda_lock etc. Combine with execute_all to also run.\nFalse\n\n\nexecute_all\nbool\nRun all included commands, overriding individual parameters like execute_conda_lock etc. Combine with include_all to run all commands.\nFalse\n\n\nconda_lock\nbool\nWhether to include self.run_conda_lock().\nTrue\n\n\nexecute_conda_lock\nbool\nWhether to run the generated commands via subprocess.run().\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to use self.default_kwargs params to run self.run_conda_lock().\nFalse\n\n\nlegacy_export\nbool\nWhether to add the self.legacy_export_cmd_str command.\nFalse\n\n\nexecute_legacy_export\nbool\nWhether to run the self.legacy_export_cmd_str().\nFalse\n\n\nlegacy_move\nbool\nWhether to add the self.legacy_mv_cmd_str() command.\nFalse\n\n\nexecute_legacy_move\nbool\nWhether to run the self.legacy_mv_cmd_str().\nFalse\n\n\ncmds_list\nlist[str] | None\nA list of commands to execute. If passed, these are executed prior.\nNone\n\n\nexecute_priors\nbool\nExecute commands passed in cmds_list prior to any others.\nFalse\n\n\ncmds_post_list\nlist[str] | None\nA list of commands to run after all others.\nNone\n\n\nexecute_cmds_post\nbool\nExecute commands passed in cmds_post_list after all others.\nFalse\n\n\nparent_dir_after_lock\nbool\nWhether to return to parent dir after lock command.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str] | str\nA list of commands generated, or a str of each command separated by a newline character (\\n).\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; print(conda_lock_file_manager.run(\n...     as_str=True, legacy_export=True, legacy_move=True))\nconda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml\nconda-lock render --kind explicit --platform linux-64\nmv conda-linux-64.lock .conda-linux-64.lock\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_conda_lock(execute=False, use_default_kwargs=False, parent_dir_after_lock=False, **kwargs)\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to include the self.default_kwargs in run.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.conda_lock_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock()\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(pdb=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_export(execute=False, **kwargs)\nRun self.legacy_export_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.legacy_export_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export()\n'conda-lock render --kind explicit --platform linux-64'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export(pdb=True)\n'conda-lock render --kind explicit --platform linux-64 --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_mv(execute=False)\nRun self.legacy_export_cmd_str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock .conda-linux-64.lock'\n&gt;&gt;&gt; conda_lock_file_manager.legacy_name_prefix = '../.'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock ../.conda-linux-64.lock'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html#functions",
    "href": "docs/reference/clim_recal.utils.server.html#functions",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nkwargs_to_cli_str\nConvert kwargs into a cli str.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\nrm_user\nRemove user and user home folder.\n\n\nset_and_pop_attr_kwargs\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\nclim_recal.utils.server.kwargs_to_cli_str(space_prefix=True, **kwargs)\nConvert kwargs into a cli str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nkey=val parameters to concatenate as str.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA final str of concatenated **kwargs in command line form.\n\n\n\n\n\n\n&gt;&gt;&gt; kwargs_to_cli_str(cat=4, in_a=\"hat\", fun=False)\n' --cat 4 --in-a hat --not-fun'\n&gt;&gt;&gt; kwargs_to_cli_str(space_prefix=False, cat=4, fun=True)\n'--cat 4 --fun'\n&gt;&gt;&gt; kwargs_to_cli_str()\n''\n\n\n\n\nclim_recal.utils.server.make_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nName for user and home folder name to append to user_home_path.\nrequired\n\n\npassword\nstr\nLogin password.\nrequired\n\n\ncode_path\nPathLike\nPath to copy code from to user home directory.\nRSTUDIO_DOCKER_USER_PATH\n\n\nuser_home_path\nPathLike\nPath that user folder will be in, often Path('/home') in linux.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPath\nFull path to generated user home folder.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'an_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = JUPYTER_DOCKER_USER_PATH\n&gt;&gt;&gt; make_user(user_name, password, code_path=code_path)\nPosixPath('/home/an_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'an_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.make_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\nPathLike\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\nCallable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; tmp_data_path = getfixture('data_fixtures_path')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = tmp_data_path / 'test_user_accounts.xlsx'\n&gt;&gt;&gt; assert test_accounts_path.exists()\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file()\n...  for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\nclim_recal.utils.server.rm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\nPathLike\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nuser name of account and home folder deleted.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.set_and_pop_attr_kwargs(instance, **kwargs)\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nAny\nAn object to modify.\nrequired\n\n\nkwargs\n\nkey: val parameters to potentially modify instance attributes.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, Any]\nAny remaining kwargs not used to modify instance.\n\n\n\n\n\n\n&gt;&gt;&gt; kwrgs = set_and_pop_attr_kwargs(\n...    conda_lock_file_manager, env_paths=['pyproject.toml'], cat=4)\n&gt;&gt;&gt; conda_lock_file_manager.env_paths\n['pyproject.toml']\n&gt;&gt;&gt; kwrgs\n{'cat': 4}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html",
    "href": "docs/reference/clim_recal.resample.html",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "clim_recal.resample\nResample UKHADS data and UKCP18 data.\n\nUKCP18 is resampled temporally from a 360 day calendar to a standard (365/366 day) calendar and projected to British National Grid (BNG) EPSG:27700 from its original rotated polar grid.\nUKHADS is resampled spatially from 1km to 2.2km in BNG aligned with the projected UKCP18\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nCPMResampler\nCPM specific changes to HADsResampler.\n\n\nCPMResamplerManager\nClass to manage processing CPM resampling.\n\n\nHADsResampler\nManage resampling HADs datafiles for modelling.\n\n\nHADsResamplerManager\nClass to manage processing HADs resampling.\n\n\nResamblerBase\nBase class to inherit for HADs and CPM.\n\n\nResamblerManagerBase\nBase class to inherit for HADs and CPM resampler managers.\n\n\n\n\n\nclim_recal.resample.CPMResampler(self, *, input_path=RAW_CPM_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH / CPM_CROP_OUTPUT_LOCAL_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name=CPM_RAW_X_COLUMN_NAME, input_file_y_column_name=CPM_RAW_Y_COLUMN_NAME, start_index=0, stop_index=None, prior_time_series=None, _resample_func=cpm_reproject_with_standard_calendar)\nCPM specific changes to HADsResampler.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nPathLike | None\nPath to CPM files to process.\n\n\noutput\n\nPath to save processed CPM files.\n\n\ninput_files\n\nPath or Paths of NCF files to reproject.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\nresampling_func\n\nFunction to call on self.input_files.\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\n\nCoordinate Reference System (CRS) to return final format in.\n\n\ninput_file_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ninput_file_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\n\nFile extensions to glob input_files with.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_cpm_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     input_path=RAW_CPM_TASMAX_PATH,\n...     output_path=resample_test_cpm_output_path,\n...     input_file_extension=TIF_EXTENSION_STR,\n... )\n&gt;&gt;&gt; cpm_resampler\n&lt;CPMResampler(...count=100,...\n    ...input_path='.../tasmax/05/latest',...\n    ...output_path='.../test-run-results_..._.../cpm')&gt;\n&gt;&gt;&gt; pprint(cpm_resampler.input_files)\n(...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_19801201-19811130.tif'),\n ...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_19811201-19821130.tif'),\n ...\n ...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_20791201-20801130.tif'))\n\n\n\n\nclim_recal.resample.CPMResamplerManager(self, *, input_paths=RAW_CPM_PATH, resample_paths=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=Path(), sub_path=CPM_SUB_PATH, start_index=0, stop_index=None, start_date=CPM_START_DATE, end_date=CPM_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=CPMResampler, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, runs=RunOptions.preferred())\nClass to manage processing CPM resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nPathLike | Sequence[PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\nresample_paths\nPathLike | Sequence[PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\n\nWhich VariableOptions to include.\n\n\nruns\nSequence[RunOptions | str]\nWhich RunOptions to include.\n\n\ncrop_regions\n\nRegionOptions (like Manchester, Scotland etc.) to crop results to.\n\n\ncrop_paths\n\nWhere to save region crop files.\n\n\nsub_path\nPath\nPath to include at the stem of input_paths.\n\n\nstart_index\n\nIndex to begin iterating input files for resampling or cropping.\n\n\nstop_index\n\nIndex to to run from start_index to when resampling or cropping. If None, iterate full list of paths.\n\n\nstart_date\ndate\nNot yet implemented, but in future from what date to generate start index from.\n\n\nend_date\ndate\nNot yet implemented, but in future from what date to generate stop index from.\n\n\nconfigs\nlist[CPMResampler]\nList of HADsResampler instances to iterate resampling or cropping.\n\n\nconfig_default_kwargs\n\nParameters passed to all running self.configs.\n\n\nresampler_class\ntype[CPMResampler]\nclass to construct all self.configs instances with.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_cpm_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler_manager: CPMResamplerManager = CPMResamplerManager(\n...     stop_index=9,\n...     resample_paths=resample_test_cpm_output_path,\n...     crop_paths=resample_test_cpm_output_path,\n...     )\n&gt;&gt;&gt; cpm_resampler_manager\n&lt;CPMResamplerManager(variables_count=1, runs_count=4,\n                     input_paths_count=4)&gt;\n&gt;&gt;&gt; configs: tuple[CPMResampler, ...] = tuple(\n...     cpm_resampler_manager.yield_configs())\n&gt;&gt;&gt; pprint(configs)\n(&lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/05/latest',\n               output_path='.../cpm/tasmax/05')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/06/latest',\n               output_path='.../cpm/tasmax/06')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/07/latest',\n               output_path='.../cpm/tasmax/07')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/08/latest',\n               output_path='.../cpm/tasmax/08')&gt;)\n\n\n\n\nclim_recal.resample.HADsResampler(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH / HADS_CROP_OUTPUT_LOCAL_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None, cpm_for_coord_alignment=RAW_CPM_TASMAX_PATH, cpm_for_coord_alignment_path_converted=False, _resample_func=hads_resample_and_reproject)\nManage resampling HADs datafiles for modelling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nPathLike | None\nPath to HADs files to process.\n\n\noutput\n\nPath to save processed HADS files.\n\n\ninput_files\nIterable[PathLike] | None\nPath or Paths of NCF files to resample.\n\n\nresampling_func\n\nFunction to call on self.input_files.\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\nstr\nCoordinate Reference System (CRS) to return final format in.\n\n\ninput_file_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ninput_file_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\nNETCDF_OR_TIF\nFile extensions to glob input_files with.\n\n\nstart_index\n\nFirst index of file to iterate processing from.\n\n\nstop_index\n\nLast index of files to iterate processing from as a count from start_index. If None, this will simply iterate over all available files.\n\n\ncpm_for_coord_alignment\nT_Dataset | PathLike | None\nCPM Path or Dataset to match alignment with.\n\n\ncpm_for_coord_alignment_path_converted\nbool\nWhether a Path passed to cpm_for_coord_alignment should be processed.\n\n\n\n\n\n\n\nTry time projection first\nCombine with space (this worked)\nAdd crop step\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_hads_output_path')\n&gt;&gt;&gt; hads_resampler: HADsResampler = HADsResampler(\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; hads_resampler\n&lt;HADsResampler(...count=504,...\n    ...input_path='.../tasmax/day',...\n    ...output_path='...run-results_..._.../hads')&gt;\n&gt;&gt;&gt; pprint(hads_resampler.input_files)\n(...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800101-19800131.nc'),\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800201-19800229.nc'),\n ...,\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_20211201-20211231.nc'))\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\n\n\nclim_recal.resample.HADsResampler.set_cpm_for_coord_alignment()\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager(self, *, input_paths=RAW_HADS_PATH, resample_paths=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=RESAMPLING_OUTPUT_PATH / HADS_CROP_OUTPUT_LOCAL_PATH, sub_path=HADS_SUB_PATH, start_index=0, stop_index=None, start_date=HADS_START_DATE, end_date=HADS_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=HADsResampler, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, cpm_for_coord_alignment=RAW_CPM_TASMAX_PATH, cpm_for_coord_alignment_path_converted=False)\nClass to manage processing HADs resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nPathLike | Sequence[PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\nresample_paths\nPathLike | Sequence[PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\n\nWhich VariableOptions to include.\n\n\ncrop_regions\n\nRegionOptions (like Manchester, Scotland etc.) to crop results to.\n\n\ncrop_paths\nSequence[PathLike] | PathLike\nWhere to save region crop files.\n\n\nsub_path\nPath\nPath to include at the stem of input_paths.\n\n\nstart_index\n\nIndex to begin iterating input files for resampling or cropping.\n\n\nstop_index\n\nIndex to to run from start_index to when resampling or cropping. If None, iterate full list of paths.\n\n\nstart_date\ndate\nNot yet implemented, but in future from what date to generate start index from.\n\n\nend_date\ndate\nNot yet implemented, but in future from what date to generate stop index from.\n\n\nconfigs\nlist[HADsResampler]\nList of HADsResampler instances to iterate resampling or cropping.\n\n\nconfig_default_kwargs\ndict[str, Any]\nParameters passed to all running self.configs.\n\n\nresampler_class\ntype[HADsResampler]\nclass to construct all self.configs instances with.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\ncpm_for_coord_alignment\nT_Dataset | PathLike\nCPM Path or Dataset to match alignment with.\n\n\ncpm_for_coord_alignment_path_converted\nbool\nWhether a Path passed to cpm_for_coord_alignment should be processed.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_hads_output_path')\n&gt;&gt;&gt; hads_resampler_manager: HADsResamplerManager = HADsResamplerManager(\n...     variables=VariableOptions.all(),\n...     resample_paths=resample_test_hads_output_path,\n...     )\n&gt;&gt;&gt; hads_resampler_manager\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.set_cpm_for_coord_alignment()\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\nclim_recal.resample.HADsResamplerManager.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerBase(self, *, input_path=Path(), output_path=RESAMPLING_OUTPUT_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name='', input_file_y_column_name='', start_index=0, stop_index=None)\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nName\nDescription\n\n\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncrop_projection\nCrop a projection to region geometry.\n\n\nexecute\nRun all steps for processing\n\n\nexecute_crops\nRun all specified crops.\n\n\nset_input_files\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerBase.crop_projection(index=0, override_export_path=None, return_results=False, sync_reprojection_paths=True, **kwargs)\nCrop a projection to region geometry.\n\n\n\nclim_recal.resample.ResamblerBase.execute(skip_spatial=False, **kwargs)\nRun all steps for processing\n\n\n\nclim_recal.resample.ResamblerBase.execute_crops(skip_crop=False, **kwargs)\nRun all specified crops.\n\n\n\nclim_recal.resample.ResamblerBase.set_input_files(new_input_path=None)\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase(self, *, input_paths=Path(), resample_paths=Path(), variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=Path(), sub_path=Path(), start_index=0, stop_index=None, start_date=None, end_date=None, configs=list(), config_default_kwargs=dict(), resampler_class=None, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False)\nBase class to inherit for HADs and CPM resampler managers.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncrop_folder\nReturn self._output_path set by set_resample_paths().\n\n\ninput_folder\nReturn self._input_path set by set_input_paths().\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\nresample_folder\nReturn self._output_path set by set_resample_paths().\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nVarirableInBaseImportPathError\nChecking import path validity for self.variables.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.VarirableInBaseImportPathError()\nChecking import path validity for self.variables.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_paths\nCheck and set input, resample and crop paths.\n\n\nexecute_crop_configs\nRun all resampler configurations\n\n\nexecute_resample_configs\nRun all resampler configurations\n\n\nset_crop_paths\nPropagate self.resample_paths if needed.\n\n\nset_resample_paths\nPropagate self.resample_paths if needed.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\nyield_crop_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.check_paths(run_set_data_paths=True, run_set_crop_paths=True)\nCheck and set input, resample and crop paths.\n\n\n\nclim_recal.resample.ResamblerManagerBase.execute_crop_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.execute_resample_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.set_crop_paths()\nPropagate self.resample_paths if needed.\n\n\n\nclim_recal.resample.ResamblerManagerBase.set_resample_paths()\nPropagate self.resample_paths if needed.\n\n\n\nclim_recal.resample.ResamblerManagerBase.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\nclim_recal.resample.ResamblerManagerBase.yield_crop_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nreproject_2_2km_filename\nReturn tweaked path to indicate standard day projection.\n\n\nreproject_standard_calendar_filename\nReturn tweaked path to indicate standard day projection.\n\n\n\n\n\nclim_recal.resample.reproject_2_2km_filename(path)\nReturn tweaked path to indicate standard day projection.\n\n\n\nclim_recal.resample.reproject_standard_calendar_filename(path)\nReturn tweaked path to indicate standard day projection.",
    "crumbs": [
      "API Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html#classes",
    "href": "docs/reference/clim_recal.resample.html#classes",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCPMResampler\nCPM specific changes to HADsResampler.\n\n\nCPMResamplerManager\nClass to manage processing CPM resampling.\n\n\nHADsResampler\nManage resampling HADs datafiles for modelling.\n\n\nHADsResamplerManager\nClass to manage processing HADs resampling.\n\n\nResamblerBase\nBase class to inherit for HADs and CPM.\n\n\nResamblerManagerBase\nBase class to inherit for HADs and CPM resampler managers.\n\n\n\n\n\nclim_recal.resample.CPMResampler(self, *, input_path=RAW_CPM_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH / CPM_CROP_OUTPUT_LOCAL_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name=CPM_RAW_X_COLUMN_NAME, input_file_y_column_name=CPM_RAW_Y_COLUMN_NAME, start_index=0, stop_index=None, prior_time_series=None, _resample_func=cpm_reproject_with_standard_calendar)\nCPM specific changes to HADsResampler.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nPathLike | None\nPath to CPM files to process.\n\n\noutput\n\nPath to save processed CPM files.\n\n\ninput_files\n\nPath or Paths of NCF files to reproject.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\nresampling_func\n\nFunction to call on self.input_files.\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\n\nCoordinate Reference System (CRS) to return final format in.\n\n\ninput_file_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ninput_file_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\n\nFile extensions to glob input_files with.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_cpm_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     input_path=RAW_CPM_TASMAX_PATH,\n...     output_path=resample_test_cpm_output_path,\n...     input_file_extension=TIF_EXTENSION_STR,\n... )\n&gt;&gt;&gt; cpm_resampler\n&lt;CPMResampler(...count=100,...\n    ...input_path='.../tasmax/05/latest',...\n    ...output_path='.../test-run-results_..._.../cpm')&gt;\n&gt;&gt;&gt; pprint(cpm_resampler.input_files)\n(...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_19801201-19811130.tif'),\n ...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_19811201-19821130.tif'),\n ...\n ...Path('.../tasmax/05/latest/tasmax_...-cpm_uk_2.2km_05_day_20791201-20801130.tif'))\n\n\n\n\nclim_recal.resample.CPMResamplerManager(self, *, input_paths=RAW_CPM_PATH, resample_paths=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=Path(), sub_path=CPM_SUB_PATH, start_index=0, stop_index=None, start_date=CPM_START_DATE, end_date=CPM_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=CPMResampler, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, runs=RunOptions.preferred())\nClass to manage processing CPM resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nPathLike | Sequence[PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\nresample_paths\nPathLike | Sequence[PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\n\nWhich VariableOptions to include.\n\n\nruns\nSequence[RunOptions | str]\nWhich RunOptions to include.\n\n\ncrop_regions\n\nRegionOptions (like Manchester, Scotland etc.) to crop results to.\n\n\ncrop_paths\n\nWhere to save region crop files.\n\n\nsub_path\nPath\nPath to include at the stem of input_paths.\n\n\nstart_index\n\nIndex to begin iterating input files for resampling or cropping.\n\n\nstop_index\n\nIndex to to run from start_index to when resampling or cropping. If None, iterate full list of paths.\n\n\nstart_date\ndate\nNot yet implemented, but in future from what date to generate start index from.\n\n\nend_date\ndate\nNot yet implemented, but in future from what date to generate stop index from.\n\n\nconfigs\nlist[CPMResampler]\nList of HADsResampler instances to iterate resampling or cropping.\n\n\nconfig_default_kwargs\n\nParameters passed to all running self.configs.\n\n\nresampler_class\ntype[CPMResampler]\nclass to construct all self.configs instances with.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_cpm_output_path: Path = getfixture(\n...         'resample_test_cpm_output_path')\n&gt;&gt;&gt; cpm_resampler_manager: CPMResamplerManager = CPMResamplerManager(\n...     stop_index=9,\n...     resample_paths=resample_test_cpm_output_path,\n...     crop_paths=resample_test_cpm_output_path,\n...     )\n&gt;&gt;&gt; cpm_resampler_manager\n&lt;CPMResamplerManager(variables_count=1, runs_count=4,\n                     input_paths_count=4)&gt;\n&gt;&gt;&gt; configs: tuple[CPMResampler, ...] = tuple(\n...     cpm_resampler_manager.yield_configs())\n&gt;&gt;&gt; pprint(configs)\n(&lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/05/latest',\n               output_path='.../cpm/tasmax/05')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/06/latest',\n               output_path='.../cpm/tasmax/06')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/07/latest',\n               output_path='.../cpm/tasmax/07')&gt;,\n &lt;CPMResampler(count=9, max_count=100,\n               input_path='.../tasmax/08/latest',\n               output_path='.../cpm/tasmax/08')&gt;)\n\n\n\n\nclim_recal.resample.HADsResampler(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH / HADS_CROP_OUTPUT_LOCAL_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None, cpm_for_coord_alignment=RAW_CPM_TASMAX_PATH, cpm_for_coord_alignment_path_converted=False, _resample_func=hads_resample_and_reproject)\nManage resampling HADs datafiles for modelling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nPathLike | None\nPath to HADs files to process.\n\n\noutput\n\nPath to save processed HADS files.\n\n\ninput_files\nIterable[PathLike] | None\nPath or Paths of NCF files to resample.\n\n\nresampling_func\n\nFunction to call on self.input_files.\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\nstr\nCoordinate Reference System (CRS) to return final format in.\n\n\ninput_file_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ninput_file_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\nNETCDF_OR_TIF\nFile extensions to glob input_files with.\n\n\nstart_index\n\nFirst index of file to iterate processing from.\n\n\nstop_index\n\nLast index of files to iterate processing from as a count from start_index. If None, this will simply iterate over all available files.\n\n\ncpm_for_coord_alignment\nT_Dataset | PathLike | None\nCPM Path or Dataset to match alignment with.\n\n\ncpm_for_coord_alignment_path_converted\nbool\nWhether a Path passed to cpm_for_coord_alignment should be processed.\n\n\n\n\n\n\n\nTry time projection first\nCombine with space (this worked)\nAdd crop step\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_hads_output_path')\n&gt;&gt;&gt; hads_resampler: HADsResampler = HADsResampler(\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; hads_resampler\n&lt;HADsResampler(...count=504,...\n    ...input_path='.../tasmax/day',...\n    ...output_path='...run-results_..._.../hads')&gt;\n&gt;&gt;&gt; pprint(hads_resampler.input_files)\n(...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800101-19800131.nc'),\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800201-19800229.nc'),\n ...,\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_20211201-20211231.nc'))\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\n\n\nclim_recal.resample.HADsResampler.set_cpm_for_coord_alignment()\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager(self, *, input_paths=RAW_HADS_PATH, resample_paths=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=RESAMPLING_OUTPUT_PATH / HADS_CROP_OUTPUT_LOCAL_PATH, sub_path=HADS_SUB_PATH, start_index=0, stop_index=None, start_date=HADS_START_DATE, end_date=HADS_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=HADsResampler, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, cpm_for_coord_alignment=RAW_CPM_TASMAX_PATH, cpm_for_coord_alignment_path_converted=False)\nClass to manage processing HADs resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nPathLike | Sequence[PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\nresample_paths\nPathLike | Sequence[PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\n\nWhich VariableOptions to include.\n\n\ncrop_regions\n\nRegionOptions (like Manchester, Scotland etc.) to crop results to.\n\n\ncrop_paths\nSequence[PathLike] | PathLike\nWhere to save region crop files.\n\n\nsub_path\nPath\nPath to include at the stem of input_paths.\n\n\nstart_index\n\nIndex to begin iterating input files for resampling or cropping.\n\n\nstop_index\n\nIndex to to run from start_index to when resampling or cropping. If None, iterate full list of paths.\n\n\nstart_date\ndate\nNot yet implemented, but in future from what date to generate start index from.\n\n\nend_date\ndate\nNot yet implemented, but in future from what date to generate stop index from.\n\n\nconfigs\nlist[HADsResampler]\nList of HADsResampler instances to iterate resampling or cropping.\n\n\nconfig_default_kwargs\ndict[str, Any]\nParameters passed to all running self.configs.\n\n\nresampler_class\ntype[HADsResampler]\nclass to construct all self.configs instances with.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\ncpm_for_coord_alignment\nT_Dataset | PathLike\nCPM Path or Dataset to match alignment with.\n\n\ncpm_for_coord_alignment_path_converted\nbool\nWhether a Path passed to cpm_for_coord_alignment should be processed.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; resample_test_hads_output_path: Path = getfixture(\n...         'resample_test_hads_output_path')\n&gt;&gt;&gt; hads_resampler_manager: HADsResamplerManager = HADsResamplerManager(\n...     variables=VariableOptions.all(),\n...     resample_paths=resample_test_hads_output_path,\n...     )\n&gt;&gt;&gt; hads_resampler_manager\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.set_cpm_for_coord_alignment()\nCheck if cpm_for_coord_alignment is a Dataset, process if a Path.\n\n\n\nclim_recal.resample.HADsResamplerManager.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerBase(self, *, input_path=Path(), output_path=RESAMPLING_OUTPUT_PATH, variable_name=VariableOptions.default(), input_files=None, cpus=None, crop_region=RegionOptions.GLASGOW, crop_path=RESAMPLING_OUTPUT_PATH, final_crs=BRITISH_NATIONAL_GRID_EPSG, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, input_file_x_column_name='', input_file_y_column_name='', start_index=0, stop_index=None)\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nName\nDescription\n\n\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncrop_projection\nCrop a projection to region geometry.\n\n\nexecute\nRun all steps for processing\n\n\nexecute_crops\nRun all specified crops.\n\n\nset_input_files\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerBase.crop_projection(index=0, override_export_path=None, return_results=False, sync_reprojection_paths=True, **kwargs)\nCrop a projection to region geometry.\n\n\n\nclim_recal.resample.ResamblerBase.execute(skip_spatial=False, **kwargs)\nRun all steps for processing\n\n\n\nclim_recal.resample.ResamblerBase.execute_crops(skip_crop=False, **kwargs)\nRun all specified crops.\n\n\n\nclim_recal.resample.ResamblerBase.set_input_files(new_input_path=None)\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase(self, *, input_paths=Path(), resample_paths=Path(), variables=(VariableOptions.default()), crop_regions=RegionOptions.all(), crop_paths=Path(), sub_path=Path(), start_index=0, stop_index=None, start_date=None, end_date=None, configs=list(), config_default_kwargs=dict(), resampler_class=None, cpus=None, _input_path_dict=dict(), _resampled_path_dict=dict(), _cropped_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False)\nBase class to inherit for HADs and CPM resampler managers.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncrop_folder\nReturn self._output_path set by set_resample_paths().\n\n\ninput_folder\nReturn self._input_path set by set_input_paths().\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\nresample_folder\nReturn self._output_path set by set_resample_paths().\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nVarirableInBaseImportPathError\nChecking import path validity for self.variables.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.VarirableInBaseImportPathError()\nChecking import path validity for self.variables.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_paths\nCheck and set input, resample and crop paths.\n\n\nexecute_crop_configs\nRun all resampler configurations\n\n\nexecute_resample_configs\nRun all resampler configurations\n\n\nset_crop_paths\nPropagate self.resample_paths if needed.\n\n\nset_resample_paths\nPropagate self.resample_paths if needed.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\nyield_crop_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.check_paths(run_set_data_paths=True, run_set_crop_paths=True)\nCheck and set input, resample and crop paths.\n\n\n\nclim_recal.resample.ResamblerManagerBase.execute_crop_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.execute_resample_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerManagerBase.set_crop_paths()\nPropagate self.resample_paths if needed.\n\n\n\nclim_recal.resample.ResamblerManagerBase.set_resample_paths()\nPropagate self.resample_paths if needed.\n\n\n\nclim_recal.resample.ResamblerManagerBase.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\nclim_recal.resample.ResamblerManagerBase.yield_crop_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.",
    "crumbs": [
      "API Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html#functions",
    "href": "docs/reference/clim_recal.resample.html#functions",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nreproject_2_2km_filename\nReturn tweaked path to indicate standard day projection.\n\n\nreproject_standard_calendar_filename\nReturn tweaked path to indicate standard day projection.\n\n\n\n\n\nclim_recal.resample.reproject_2_2km_filename(path)\nReturn tweaked path to indicate standard day projection.\n\n\n\nclim_recal.resample.reproject_standard_calendar_filename(path)\nReturn tweaked path to indicate standard day projection.",
    "crumbs": [
      "API Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.config.html",
    "href": "docs/reference/clim_recal.config.html",
    "title": "1 clim_recal.config",
    "section": "",
    "text": "clim_recal.config\n\n\n\n\n\nName\nDescription\n\n\n\n\nBaseRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalRunsConfigType\nLists of parameters to generate RunConfigType instances.\n\n\nRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfigType\nParameters needed for a model run.\n\n\n\n\n\nclim_recal.config.BaseRunConfig(self)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.config.ClimRecalConfig(self, variables=(VariableOptions.default()), runs=(RunOptions.default()), regions=(RegionOptions.default()), methods=(MethodOptions.default()), multiprocess=False, cpus=DEFAULT_CPUS, hads_input_path=RAW_HADS_PATH, cpm_input_path=RAW_CPM_PATH, output_path=DEFAULT_OUTPUT_PATH, resample_folder=DEFAULT_RESAMPLE_FOLDER, crops_folder=DEFAULT_CROPS_FOLDER, hads_output_folder=HADS_OUTPUT_LOCAL_PATH, cpm_output_folder=CPM_OUTPUT_LOCAL_PATH, cpm_kwargs=dict(), hads_kwargs=dict(), start_index=0, stop_index=None, add_local_dated_results_path=True, add_local_dated_crops_path=True, local_dated_results_path_prefix='run', local_dated_crops_path_prefix='crop', cpm_for_coord_alignment=None, process_cmp_for_coord_alignment=False, cpm_for_coord_alignment_path_converted=False, debug_mode=False)\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvariables\nSequence[VariableOptions]\nVariables to include in the model, eg. tasmax, tasmin.\n\n\nruns\nSequence[RunOptions]\nWhich model runs to include, eg. “01”, “08”, “11”.\n\n\nregions\nSequence[RegionOptions] | None\nWhich regions to crop both HADs and CPM data to.\n\n\nmethods\nSequence[MethodOptions]\nWhich debiasing methods to apply.\n\n\nmultiprocess\nbool\nWhether to use multiprocess where available\n\n\ncpus\nint | None\nNumber of cpus to use if multiprocessing\n\n\noutput_path\nPathLike\nPath to save all intermediate and final results to.\n\n\nresample_folder\nPathLike\nPath to append to output_path for resampling result files.\n\n\ncrops_folder\nPathLike\nPath to append to output_path for cropped resample files.\n\n\nhads_output_folder\nPathLike\nPath to append to output_path / resample_folder for resampling HADs files and to output_path / crop_folder for crops.\n\n\ncpm_output_folder\nPathLike\nPath to append to output_path / resample_folder for resampling CPM files and to output_path / crop_folder for crops.\n\n\ncpm_kwargs\ndict\nA dict of parameters to pass to a CPMResamplerManager.\n\n\nhads_kwargs\ndict\nA dict of parameters to pass to HADsResamplerManager.\n\n\ncpm_for_coord_alignment\nPathLike | None\nA Path to a CPM file to align HADs coordinates to.\n\n\ndebug_mode\nbool\nSet to True to add more detailed debug logs, including GDAL.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; run_config: ClimRecalConfig = ClimRecalConfig(\n...     regions=('Manchester', 'Glasgow'),\n...     output_path=test_runs_output_path,\n...     cpus=1)\n&gt;&gt;&gt; run_config\n&lt;ClimRecalConfig(variables_count=1, runs_count=1, regions_count=2,\n                 methods_count=1, cpm_folders_count=1,\n                 hads_folders_count=1, start_index=0,\n                 stop_index=None, cpus=1)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nIf cpm_for_coord_alignment is None use self.cpm_input_path.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig.set_cpm_for_coord_alignment()\nIf cpm_for_coord_alignment is None use self.cpm_input_path.\nIt would be more efficient to use self.resample_cpm_path as long as that option is used, but support cases of only\n\n\n\n\n\nclim_recal.config.ClimRecalRunsConfigType()\nLists of parameters to generate RunConfigType instances.\n\n\n\nclim_recal.config.RunConfig(self)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.config.RunConfigType()\nParameters needed for a model run.",
    "crumbs": [
      "API Reference",
      "Configure"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.config.html#classes",
    "href": "docs/reference/clim_recal.config.html#classes",
    "title": "1 clim_recal.config",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBaseRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalRunsConfigType\nLists of parameters to generate RunConfigType instances.\n\n\nRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfigType\nParameters needed for a model run.\n\n\n\n\n\nclim_recal.config.BaseRunConfig(self)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.config.ClimRecalConfig(self, variables=(VariableOptions.default()), runs=(RunOptions.default()), regions=(RegionOptions.default()), methods=(MethodOptions.default()), multiprocess=False, cpus=DEFAULT_CPUS, hads_input_path=RAW_HADS_PATH, cpm_input_path=RAW_CPM_PATH, output_path=DEFAULT_OUTPUT_PATH, resample_folder=DEFAULT_RESAMPLE_FOLDER, crops_folder=DEFAULT_CROPS_FOLDER, hads_output_folder=HADS_OUTPUT_LOCAL_PATH, cpm_output_folder=CPM_OUTPUT_LOCAL_PATH, cpm_kwargs=dict(), hads_kwargs=dict(), start_index=0, stop_index=None, add_local_dated_results_path=True, add_local_dated_crops_path=True, local_dated_results_path_prefix='run', local_dated_crops_path_prefix='crop', cpm_for_coord_alignment=None, process_cmp_for_coord_alignment=False, cpm_for_coord_alignment_path_converted=False, debug_mode=False)\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvariables\nSequence[VariableOptions]\nVariables to include in the model, eg. tasmax, tasmin.\n\n\nruns\nSequence[RunOptions]\nWhich model runs to include, eg. “01”, “08”, “11”.\n\n\nregions\nSequence[RegionOptions] | None\nWhich regions to crop both HADs and CPM data to.\n\n\nmethods\nSequence[MethodOptions]\nWhich debiasing methods to apply.\n\n\nmultiprocess\nbool\nWhether to use multiprocess where available\n\n\ncpus\nint | None\nNumber of cpus to use if multiprocessing\n\n\noutput_path\nPathLike\nPath to save all intermediate and final results to.\n\n\nresample_folder\nPathLike\nPath to append to output_path for resampling result files.\n\n\ncrops_folder\nPathLike\nPath to append to output_path for cropped resample files.\n\n\nhads_output_folder\nPathLike\nPath to append to output_path / resample_folder for resampling HADs files and to output_path / crop_folder for crops.\n\n\ncpm_output_folder\nPathLike\nPath to append to output_path / resample_folder for resampling CPM files and to output_path / crop_folder for crops.\n\n\ncpm_kwargs\ndict\nA dict of parameters to pass to a CPMResamplerManager.\n\n\nhads_kwargs\ndict\nA dict of parameters to pass to HADsResamplerManager.\n\n\ncpm_for_coord_alignment\nPathLike | None\nA Path to a CPM file to align HADs coordinates to.\n\n\ndebug_mode\nbool\nSet to True to add more detailed debug logs, including GDAL.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; run_config: ClimRecalConfig = ClimRecalConfig(\n...     regions=('Manchester', 'Glasgow'),\n...     output_path=test_runs_output_path,\n...     cpus=1)\n&gt;&gt;&gt; run_config\n&lt;ClimRecalConfig(variables_count=1, runs_count=1, regions_count=2,\n                 methods_count=1, cpm_folders_count=1,\n                 hads_folders_count=1, start_index=0,\n                 stop_index=None, cpus=1)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_cpm_for_coord_alignment\nIf cpm_for_coord_alignment is None use self.cpm_input_path.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig.set_cpm_for_coord_alignment()\nIf cpm_for_coord_alignment is None use self.cpm_input_path.\nIt would be more efficient to use self.resample_cpm_path as long as that option is used, but support cases of only\n\n\n\n\n\nclim_recal.config.ClimRecalRunsConfigType()\nLists of parameters to generate RunConfigType instances.\n\n\n\nclim_recal.config.RunConfig(self)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.config.RunConfigType()\nParameters needed for a model run.",
    "crumbs": [
      "API Reference",
      "Configure"
    ]
  },
  {
    "objectID": "docs/download.html",
    "href": "docs/download.html",
    "title": "Dataset Links",
    "section": "",
    "text": "Thanks for your interest in clim-recal data and metrics. Below are instructions for downloading observation and simulation datasets for three cities in the UK (Glasgow, Manchester and London) and Scotland. We invite authors of bias correction methods to benchmark their methods for those cities.\nSince there are 15,060 files that make up the dataset, and not all users will require all files, there is a text file providing URLs for each of the data files. There is also an associated manifest file that can be used for validating the downloaded data.\nBelow are some shell commands that can be can be passed to the command-line to download subsets of the data as required having first downloaded the full list of file URLs (data-v1.0.txt), that assume some familiary with the linux-like command-line.\nPlease also see our blog article describing this work.",
    "crumbs": [
      "Download Datasets"
    ]
  },
  {
    "objectID": "docs/download.html#resample",
    "href": "docs/download.html#resample",
    "title": "Dataset Links",
    "section": "1 Resample",
    "text": "1 Resample\n\n1.1 HAD\nFor a given measurement &lt;MEASURE&gt; (either tasmax, tasmin or rainfall), the monthly data can be downloaded and decompressed with:\ngrep -iE \"resample.*hads.*&lt;MEASURE&gt;.*_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\nFor example, for rainfall:\ngrep -iE \"resample.*hads.*rainfall.*_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\n\n\n1.2 CPM\nFor a given measurement &lt;MEASURE&gt; (either tasmax, tasmin or pr), for run &lt;RUN&gt; (either 01, 05, 06, 07, 08), the yearly data can be downloaded and decompressed with:\ngrep -iE \"resample.*cpm.*&lt;MEASURE&gt;.*&lt;RUN&gt;.*_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\nFor exmample, for rainfall and 01:\ngrep -iE \"resample.*cpm.*rainfall.*01.*_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz",
    "crumbs": [
      "Download Datasets"
    ]
  },
  {
    "objectID": "docs/download.html#crops",
    "href": "docs/download.html#crops",
    "title": "Dataset Links",
    "section": "2 Crops",
    "text": "2 Crops\n\n2.1 HADS\nFor a given region &lt;REGION&gt; (either Scotland, Glasgow, Manchester or London), for measurement &lt;MEASURE&gt; (either tasmax, tasmin or pr), the monthly data can be downloaded and decompressed with:\ngrep -iE \"crop.*hads.*&lt;REGION&gt;.*&lt;MEASURE&gt;.*&lt;RUN&gt;_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\nFor example, for region is Manchester, measure is tasmax:\ngrep -iE \".*crop.*hads.*manchester.*tasmax.*_[0-9]{8}-[0-9]{8}\\.nc\\.gz\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\n\n\n2.2 CPM\nFor a given region &lt;REGION&gt; (either Scotland, Glasgow, Manchester or London), for measurement &lt;MEASURE&gt; (either tasmax, tasmin or pr), for run &lt;RUN&gt; (either 01, 05, 06, 07, 08), the yearly data can be downloaded and decompressed with:\ngrep -iE \"crop.*cpm.*&lt;REGION&gt;.*&lt;MEASURE&gt;.*&lt;RUN&gt;_[0-9]{8}-[0-9]{8}.*\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz\nFor example, for region Manchester, measure tasmax, run 01:\ngrep -iE \".*crop.*cpm.*manchester.*tasmax.*01_[0-9]{8}-[0-9]{8}\\.nc\\.gz\" data-v1.0.txt | xargs -n 1 curl -O; gunzip *.nc.gz",
    "crumbs": [
      "Download Datasets"
    ]
  },
  {
    "objectID": "notebooks/cpm_projection_diff_plots_linear_nearest.html",
    "href": "notebooks/cpm_projection_diff_plots_linear_nearest.html",
    "title": "clim-recal",
    "section": "",
    "text": "import xarray\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.axes import Axes\nimport pandas as pd\n\nRUN_VARS = [\"01\", \"05\", \"06\", \"07\", \"08\"]\nVARS = [\"tasmax\", \"tasmin\", \"pr\"]\n\n\ndef get_root_filepath():\n    return \"https://climrecal.blob.core.windows.net/analysis/cpm-median-time-series\"\n\n\ndef add_cols(df):\n    df[\"day\"] = df[\"time\"].apply(lambda x: x.timetuple().tm_yday)\n    df[\"month\"] = df[\"time\"].apply(lambda x: x.month)\n    df[\"year\"] = df[\"time\"].apply(lambda x: x.year)\n    df[\"day_of_month\"] = df[\"time\"].apply(lambda x: x.day)\n    df[\"leap_year\"] = df[\"time\"].apply(lambda x: x.year % 4 == 0)\n    return df\n\n\ndef get_days(is_leap_year: bool) -&gt; np.array:\n    if not is_leap_year:\n        # https://docs.xarray.dev/en/stable/generated/xarray.Dataset.convert_calendar.html\n        #  February 6th (36), April 19th (109), July 2nd (183), September 12th (255), November 25th (329).\n        # First missing day should be 37, not 36 since February 6th is 37\n        return np.array([37, 109, 183, 255, 329])\n    else:\n        # January 31st (31), March 31st (91), June 1st (153), July 31st (213), September 31st (275) and November 30th (335).\n        return np.array([31, 91, 153, 213, 275, 335])\n\n\ndef get_data() -&gt; dict[str, dict[str, tuple[pd.DataFrame, pd.DataFrame]]]:\n    data = {}\n    for var in VARS:\n        data[var] = {}\n        for run_var in RUN_VARS:\n            path_raw = f\"{get_root_filepath()}/cpm-raw-medians/median-{var}-{run_var}.nc#mode=bytes\"\n            path_con = f\"{get_root_filepath()}/cpm-converted-linear-medians/median-{var}-{run_var}.nc#mode=bytes\"\n            path_con_near = f\"{get_root_filepath()}/cpm-converted-nearest-medians/median-{var}-{run_var}.nc#mode=bytes\"\n            x_raw = xarray.load_dataset(path_raw)\n            x_con = xarray.load_dataset(path_con)\n            x_con_near = xarray.load_dataset(path_con_near)\n            df_raw = add_cols(\n                x_raw.convert_calendar(\"standard\", align_on=\"year\")\n                .to_pandas()\n                .reset_index()\n            )\n            df_con = add_cols(x_con.to_pandas().reset_index())\n            df_con_near = add_cols(x_con_near.to_pandas().reset_index())\n            data[var][run_var] = (df_raw, df_con, df_con_near)\n    return data\n\n\n# Load all data\ndata = get_data()\n\nWarning 3: Cannot find header.dxf (GDAL_DATA is not defined)\n\n\n\n# Example\nvar, run_var = \"tasmax\", \"01\"\ndf_raw, df_con, df_con_near = data[var][run_var]\n\n\n# Missing Day 37\ndf_raw.iloc[64:70]\n\n\n\n\n\n\n\n\ntime\ntasmax\nday\nmonth\nyear\nday_of_month\nleap_year\n\n\n\n\n64\n1981-02-04 12:00:00\n7.901025\n35\n2\n1981\n4\nFalse\n\n\n65\n1981-02-05 12:00:00\n9.846338\n36\n2\n1981\n5\nFalse\n\n\n66\n1981-02-07 12:00:00\n10.244776\n38\n2\n1981\n7\nFalse\n\n\n67\n1981-02-08 12:00:00\n7.123193\n39\n2\n1981\n8\nFalse\n\n\n68\n1981-02-09 12:00:00\n7.830713\n40\n2\n1981\n9\nFalse\n\n\n69\n1981-02-10 12:00:00\n7.916040\n41\n2\n1981\n10\nFalse\n\n\n\n\n\n\n\n\ndef plot_by_day(\n    data,\n    var: str,\n    run_var: str,\n    year: int,\n    leap_year: bool,\n    plot_diff: bool,\n    ax: Axes,\n    lw=0.8,\n):\n    df_raw, df_con, df_con_near = data[var][run_var]\n    if not plot_diff:\n        for i, df in enumerate([df_raw, df_con, df_con_near]):\n            if leap_year is not None:\n                df2 = df[df[\"leap_year\"].eq(leap_year)]\n            else:\n                df2 = df\n            if year is None:\n                x = df2.groupby(\"day\")[var].median()\n            else:\n                x = df2[df2[\"year\"].eq(year)].groupby(\"day\")[var].first()\n            if i == 0:\n                if leap_year is not None:\n                    ax.vlines(\n                        get_days(leap_year),\n                        x.min(),\n                        x.max(),\n                        zorder=-1,\n                        lw=0.4,\n                        color=\"k\",\n                        ls=\":\",\n                    )\n                ax.plot(x.index, x, lw=lw, alpha=0.8)\n            else:\n                ax.plot(x.index, x, lw=lw, alpha=0.8)\n                # pass\n    else:\n        series = []\n        for i, df in enumerate([df_raw, df_con, df_con_near]):\n            if leap_year is not None:\n                df2 = df[df[\"leap_year\"].eq(leap_year)]\n            else:\n                df2 = df\n            if year is None:\n                x = df2.groupby(\"day\")[var].median()\n            else:\n                x = df2[df2[\"year\"].eq(year)].groupby(\"day\")[var].first()\n            series.append(x)\n\n        # Use absolute difference\n        x2 = series[1] - series[0]\n        x3 = series[2] - series[0]\n\n        ax.vlines(\n            get_days(leap_year),\n            min([x2.min(), x3.min()]),\n            max([x2.max(), x3.max()]),\n            zorder=-1,\n            lw=0.4,\n            color=\"k\",\n            ls=\":\",\n        )\n\n        ax.plot(x2.index, x2, lw=lw, alpha=0.8, color=\"C1\", ls=\"-\")\n        ax.plot(x3.index, x3, lw=lw, alpha=0.8, color=\"C2\", ls=\"-\")\n\n    ax.set_title(f\"Year: {year}, Leap year: {leap_year}\")\n    if not plot_diff:\n        ax.set_ylabel(f\"Difference ({var})\")\n    else:\n        ax.set_ylabel(f\"Value ({var})\")\n\n\ndef plot_by_day_pair(\n    data,\n    var: str,\n    run_var: str,\n    year: int,\n    leap_year: bool,\n    axs: Axes,\n    lw=0.8,\n    series_diff_tuples: list[tuple] = [(0, 1), (0, 2)],\n):\n    ax1, ax2 = axs[0], axs[1]\n    df_raw, df_con, df_con_near = data[var][run_var]\n    series = []\n    for i, df in enumerate([df_raw, df_con, df_con_near]):\n        if leap_year is not None:\n            df2 = df[df[\"leap_year\"].eq(leap_year)]\n        else:\n            df2 = df\n        if year is None:\n            x = df2.groupby(\"day\")[var].median()\n        else:\n            x = df2[df2[\"year\"].eq(year)].groupby(\"day\")[var].first()\n        series.append(x)\n\n    # Use difference\n    x2 = series[series_diff_tuples[0][1]] - series[series_diff_tuples[0][0]]\n    x3 = series[series_diff_tuples[1][1]] - series[series_diff_tuples[1][0]]\n\n    for i, ax in enumerate([ax1, ax2]):\n        interp = \"linear\" if i == 0 else \"nearest\"\n        ax.vlines(\n            get_days(leap_year),\n            min([x2.min(), x3.min()]),\n            max([x2.max(), x3.max()]),\n            zorder=-1,\n            lw=0.4,\n            color=\"k\",\n            ls=\":\",\n        )\n        ax.set_title(\n            f\"Year: {year}, Leap year: {leap_year}, Interpolation: {interp}\",\n            fontsize=\"medium\",\n        )\n    ax1.plot(x2.index, x2, lw=lw, alpha=0.8, color=\"C1\", ls=\"-\")\n    ax2.plot(x3.index, x3, lw=lw, alpha=0.8, color=\"C2\", ls=\"-\")\n    ax1.set_ylabel(f\"Value ({var})\")\n\n\ndef plot_array(data, year, leap_year=True, plot_diff=False, lw=0.8):\n    run_vars = RUN_VARS\n    vars = VARS\n    fig, axs = plt.subplots(\n        len(vars),\n        len(run_vars),\n        squeeze=False,\n        sharex=True,\n        sharey=\"row\",\n        figsize=(14, 6),\n    )\n    for row, var in enumerate(vars):\n        for col, run_var in enumerate(run_vars):\n            ax = axs[row][col]\n            plot_by_day(data, var, run_var, year, leap_year, plot_diff, ax=ax, lw=lw)\n            if col == 0:\n                ax.set_ylabel(var, fontsize=\"medium\")\n            else:\n                ax.set_ylabel(\"\")\n            if row == 0:\n                ax.set_title(f\"Run: {run_var}\", fontsize=\"medium\")\n            else:\n                ax.set_title(\"\")\n\n\ndef plot_array_pair_diff(\n    data,\n    year,\n    leap_year=True,\n    run_var=\"01\",\n    lw=0.8,\n    series_diff_tuples: list[tuple] = [(0, 1), (0, 2)],\n):\n    run_vars = [run_var]\n    vars = VARS\n    fig, axs = plt.subplots(\n        len(vars),\n        2,\n        squeeze=False,\n        sharex=True,\n        sharey=\"row\",\n        figsize=(14, 6),\n    )\n    for row, var in enumerate(vars):\n        for col, run_var in enumerate(run_vars):\n            ax_pair = axs[row]\n            plot_by_day_pair(\n                data,\n                var,\n                run_var,\n                year,\n                leap_year,\n                axs=ax_pair,\n                lw=lw,\n                series_diff_tuples=series_diff_tuples,\n            )\n            ax_pair[0].set_ylabel(var, fontsize=\"medium\")\n\n\nplot_by_day(data, \"tasmax\", \"01\", None, leap_year=True, plot_diff=False, ax=plt.gca())\n\n\n\n\n\n\n\n\n\n# Differences between series:\n#   (orange) linear - raw\n#   (green)  nearest - raw\nplot_by_day_pair(\n    data,\n    \"tasmax\",\n    \"01\",\n    None,\n    leap_year=True,\n    axs=plt.subplots(1, 2, sharey=\"row\", figsize=(12, 5))[1],\n    series_diff_tuples=[(0, 1), (0, 2)],\n)\n\n\n\n\n\n\n\n\n\n# Differences between series:\n#   (orange) linear - raw\n#   (green)  nearest - linear\n# fig, axs = plt.subplots(1, 2, sharey=\"row\", figsize=(12, 5))\nplot_by_day_pair(\n    data,\n    \"tasmax\",\n    \"01\",\n    None,\n    leap_year=True,\n    axs=plt.subplots(1, 2, sharey=\"row\", figsize=(12, 5))[1],\n    series_diff_tuples=[(0, 1), (1, 2)],\n)\n# plt.show()\n\n\n\n\n\n\n\n\n\nplot_by_day(data, \"tasmax\", \"01\", None, leap_year=False, plot_diff=True, ax=plt.gca())\n\n\n\n\n\n\n\n\n\n# Plot arrays\nfor year in [None, 1984]:\n    for leap_year in [True, False]:\n        for plot_diff in [False, True]:\n            if year is not None:\n                actual_year = year if leap_year else 1983\n            else:\n                actual_year = None\n            print(\n                f\"Plot diff: {plot_diff}; year: {actual_year}; leap_year: {leap_year}\"\n            )\n            if not plot_diff:\n                plot_array(\n                    data, year=actual_year, leap_year=leap_year, plot_diff=plot_diff\n                )\n            else:\n                # run_var=\"01\" as an example\n                plot_array_pair_diff(\n                    data, run_var=\"01\", year=actual_year, leap_year=leap_year\n                )\n            plt.show()\n\nPlot diff: False; year: None; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: None; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: None; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: None; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: 1984; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: 1984; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: 1983; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: 1983; leap_year: False\n\n\n\n\n\n\n\n\n\n\ndef explore_year_range_xlim(\n    year_range, leap_years: bool, var=\"tasmax\", run_var=\"01\", xlim=(320, 340)\n):\n    for year in year_range:\n        if year % 4 == (1 if leap_years else 0):\n            continue\n        print(f\"raw (blue), linear (orange), nearest (green) values; year: {year}\")\n        # Explore location of spike in difference in var=\"tasmax\", run_var=\"08\"\n        plot_by_day(\n            data,\n            var,\n            run_var,\n            year=year,\n            leap_year=False,\n            plot_diff=False,\n            ax=plt.gca(),\n        )\n        # Subset days to region where spike is\n        plt.xlim(*xlim)\n        plt.show()\n        print(f\"linear - raw (orange), nearest - raw (green); year: {year}\")\n        plot_by_day(\n            data,\n            var,\n            run_var,\n            year=year,\n            leap_year=False,\n            plot_diff=True,\n            ax=plt.gca(),\n        )\n        plt.xlim(*xlim)\n        plt.show()\n\n\nexplore_year_range_xlim(list(range(1981, 1989)), leap_years=False)\n\nraw (blue), linear (orange), nearest (green) values; year: 1981\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1981\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1982\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1982\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1983\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1983\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1985\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1985\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1986\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1986\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1987\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1987\n\n\n\n\n\n\n\n\n\n\nexplore_year_range_xlim(list(range(1981, 1989)), leap_years=False)\n\nraw (blue), linear (orange), nearest (green) values; year: 1981\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1981\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1982\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1982\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1983\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1983\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1985\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1985\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1986\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1986\n\n\n\n\n\n\n\n\n\nraw (blue), linear (orange), nearest (green) values; year: 1987\n\n\n\n\n\n\n\n\n\nlinear - raw (orange), nearest - raw (green); year: 1987\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Appendix",
      "CPM Projection Analysis",
      "Nearest vs Linear Temporal Interpolation"
    ]
  },
  {
    "objectID": "python/README.html",
    "href": "python/README.html",
    "title": "1 Installation",
    "section": "",
    "text": "See setup instructions for detailed installation options, including via the python package pdm, conda and docker.\nIf the python clim-recal package is installed, some of the components can be used via the command line. For details run:\n$ clim-recal --help",
    "crumbs": [
      "Pipeline Description"
    ]
  },
  {
    "objectID": "python/README.html#download-uk-met-office-data",
    "href": "python/README.html#download-uk-met-office-data",
    "title": "1 Installation",
    "section": "2.1 Download UK Met Office data",
    "text": "2.1 Download UK Met Office data\nWe use two datasets maintained by the Met Office\n\nUKHAD observational data\nRCP8.5 projection data\n\nThis data can be quite large and take significant amounts of time to download.\nTo download this an account is needed. The registration process can be found here: https://services.ceda.ac.uk/cedasite/register/info/.\nWe hope to incorporate this module in the clim-recal command line interface in future, but at present the ceda_ftp_download.py module is used to download this data and can be run from the command line via python3:\n$ cd clim-recal/python/clim_recal\n$ ./ceda_ftp_download.py  --help\nusage: ceda_ftp_download.py [-h] --input INPUT [--output OUTPUT] --username\n                            USERNAME --psw PSW [--reverse] [--shuffle]\n                            [--change_hierarchy]\n\noptions:\n  -h, --help           show this help message and exit\n  --input INPUT        Path where the CEDA data to download is located. This can be\n                       a path with or without subdirectories. Set to\n                       `/badc/ukcp18/data/land-cpm/uk/2.2km/rcp85/` to download all\n                       the raw UKCP2.2 climate projection data used in clim-recal.\n  --output OUTPUT      Path to save the downloaded data\n  --username USERNAME  Username to connect to the CEDA servers\n  --psw PSW            FTP password to authenticate to the CEDA servers\n  --reverse            Run download in reverse (useful to run downloads in\n                       parallel)\n  --shuffle            Run download in shuffle mode (useful to run downloads in\n                       parallel)\n  --change_hierarchy   Change the output sub-directories' hierarchy to fit the\n                       Turing Azure fileshare hierarchy (only applicable to UKCP\n                       climate projection data, i.e. when --input is set to",
    "crumbs": [
      "Pipeline Description"
    ]
  },
  {
    "objectID": "python/README.html#aligning-hads-and-cpm-uk-data",
    "href": "python/README.html#aligning-hads-and-cpm-uk-data",
    "title": "1 Installation",
    "section": "2.2 Aligning HADs and CPM UK data",
    "text": "2.2 Aligning HADs and CPM UK data\nThese dataset are provided in the following forms\n\n\n\nDataset\nResolution\nCoordinates\nCalendar\n\n\n\n\nUKHAD\n1 km\nBritish National Grid1\nStandard\n\n\nRCP8.5\n2.2 km\nRotated Pole2\n360 day\n\n\n\nTo align them we:\n\nInterpolate RCP8.5 to standard Gregorian calendar via the nearest method3.\nReproject RCP8.5 to British National Grid Coordinate structure.\nResample UKHAD to a 2.2 km grid.\nReproject UKHAD bounds to align with RCP8.5\n\nLinks to these files are in the Download Datasets section.",
    "crumbs": [
      "Pipeline Description"
    ]
  },
  {
    "objectID": "python/README.html#running-debiasing-methods",
    "href": "python/README.html#running-debiasing-methods",
    "title": "1 Installation",
    "section": "2.3 Running debiasing methods",
    "text": "2.3 Running debiasing methods\nOnce the data is aligned, it is ready for you to run your choice of debiasing methods on it.",
    "crumbs": [
      "Pipeline Description"
    ]
  },
  {
    "objectID": "python/README.html#footnotes",
    "href": "python/README.html#footnotes",
    "title": "1 Installation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBritish National Grid spec: https://epsg.io/27700↩︎\nRotated pole of latitude 37.5 and longitude 177.5. See UKCP Guidance: Data availability, access and formats appendix B.↩︎\nSee the CPM Projection section for more details, including a comparison with a linear interpolation.↩︎",
    "crumbs": [
      "Pipeline Description"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "rm(list=ls())\n\nknitr::opts_knit$set(root.dir=\"/Volumes/vmfileshare/ClimateData/\")\n\nlibrary(terra)\nlibrary(sp)\nlibrary(exactextractr)\n\ndd &lt;- \"/Volumes/vmfileshare/ClimateData/\"\n\n\nBias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS\n\n\n\n\n\n\nResampling script here The 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")\n\n\n\n\nCrop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE",
    "crumbs": [
      "Analysis in R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Bias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS",
    "crumbs": [
      "Analysis in R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Resampling script here The 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")",
    "crumbs": [
      "Analysis in R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Crop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE",
    "crumbs": [
      "Analysis in R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/README.html",
    "href": "R/README.html",
    "title": "1 Methods implemented in R",
    "section": "",
    "text": "1 Methods implemented in R\n\n/Resampling - code for Resampling data to different extents (grid sizes)\n/bias-correction-methods - bias correction methods implemented in R\n/comparing-r-and-python - Comparing various pipeline aspects between R and python\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis in R"
    ]
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Welcome to clim-recal, a specialized resource designed to tackle systematic errors or biases in Regional Climate Models (RCMs). As researchers, policy-makers, and various stakeholders explore publicly available RCMs, they need to consider the challenge of biases that can affect the accurate representation of climate change signals.\nclim-recal provides both a broad review of available bias correction methods as well as software, practical tutorials and guidance that helps users apply these methods methods to various datasets.\nclim-recal is an extensive software library and guide to application of Bias Correction (BC) methods:\n\nContains accessible information about the why and how of bias correction for climate data\nIs a software library for for the application of BC methods (see our full pipeline for bias-correction of the ground-breaking local-scale (2.2km) Convection Permitting Model (CPM). clim-recal brings together different software packages in python and R that implement a variety of bias correction methods, making it easy to apply them to data and compare their outputs.\nWas developed in partnership with the MetOffice to ensure the propriety, quality, and usability of our work\nProvides a framework for open additions of new software libraries/bias correction methods (in planning)",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#user-documentation",
    "href": "README.html#user-documentation",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "3.1 User documentation",
    "text": "3.1 User documentation\n\nSee setup instructions\nSee python README for an overview of the pipeline\nOnce installed, using the clim-recal --help option for details\nSee the reproducibility page for information on how we used clim-recal",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#to-use-clim-recal-programmatically",
    "href": "README.html#to-use-clim-recal-programmatically",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "3.2 To use clim-recal programmatically",
    "text": "3.2 To use clim-recal programmatically\n\nThere are extensive API Reference within the python code.\nComments within R scripts",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#to-contribute-to-clim-recal",
    "href": "README.html#to-contribute-to-clim-recal",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "3.3 To contribute to clim-recal",
    "text": "3.3 To contribute to clim-recal\n\nSee the Contributing section below",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#ukcp18",
    "href": "README.html#ukcp18",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "4.1 UKCP18",
    "text": "4.1 UKCP18\nThe UK Climate Projections 2018 (UKCP18) dataset offers insights into the potential climate changes in the UK. UKCP18 is an advancement of the UKCP09 projections and delivers the latest evaluations of the UK’s possible climate alterations in land and marine regions throughout the 21st century. This crucial information aids in future Climate Change Risk Assessments and supports the UK’s adaptation to climate change challenges and opportunities as per the National Adaptation Programme.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#hads",
    "href": "README.html#hads",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "4.2 HADS",
    "text": "4.2 HADS\nHadUK-Grid is a comprehensive collection of climate data for the UK, compiled from various land surface observations across the country. This data is organized into a uniform grid to ensure consistent coverage throughout the UK at up to 1km x 1km resolution. The dataset, spanning from 1836 to the present, includes a variety of climate variables such as air temperature, precipitation, sunshine, and wind speed, available on daily, monthly, seasonal, and annual timescales.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#acknowledgements",
    "href": "README.html#acknowledgements",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "7.1 Acknowledgements",
    "text": "7.1 Acknowledgements\nPrior to 12th September 2024 we included a reference to the python-cmethods library, written by Benjamin Thomas Schwertfeger.\nThis was via a git submodule which targeted https://github.com/alan-turing-institute/python-cmethods, itself a fork of the original library.\nInadvertently, we did not identify that the license for the python-cmethods library (GPL3) is not compatible with the license for this package (MIT). We apologise for this mistake and have taken the following actions to resolve it:\n\nWe have now removed the relevant submodule from this repository.\nAdded this note to the README.\nAdded a note to the python/README.md file.\nAdded the citation below.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#citation",
    "href": "README.html#citation",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "7.2 Citation",
    "text": "7.2 Citation\npython-cmethods: Benjamin T. Schwertfeger. (2024). btschwertfeger/python-cmethods: v2.3.0 (v2.3.0). Zenodo. https://doi.org/10.5281/zenodo.12168002",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#footnotes",
    "href": "README.html#footnotes",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSenatore et al., 2022, https://doi.org/10.1016/j.ejrh.2022.101120↩︎\nAyar et al., 2021, https://doi.org/10.1038/s41598-021-82715-1↩︎",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "setup-instructions.html",
    "href": "setup-instructions.html",
    "title": "clim-recal installation",
    "section": "",
    "text": "clim-recal is a set of tools to manage UK climate data and projections, and running a range of correction methods in python and R. For ease of installation and use, we focus primarily on the python portion, but in principal the R notebooks should be useable via RStudio, which suggests packages necessary to install. We also provide docker configurations which include both R and python environments.",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "setup-instructions.html#conda-mamba",
    "href": "setup-instructions.html#conda-mamba",
    "title": "clim-recal installation",
    "section": "3.1 Conda / Mamba",
    "text": "3.1 Conda / Mamba\nMore detailed examples using conda or mamba are below. Installation instructions for either these are available:\n\nconda: https://conda.io/projects/conda/en/latest/user-guide/install/index.html\nmamba: https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html\n\nThese options are primarily to ease use of GDAL and optionally rsync.",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "setup-instructions.html#conda-lock",
    "href": "setup-instructions.html#conda-lock",
    "title": "clim-recal installation",
    "section": "3.2 conda-lock",
    "text": "3.2 conda-lock\nconda-lock provides a means of managing dependencies for conda/mamba environments on precise, reproducible levels, and can incorporated local pyproject.toml configurations.\nThere are many ways to install conda-lock (for most up to date options see README.md):\n\npipxpipcondamamba\n\n\npipx install conda-lock\n\n\npip install conda-lock\n\n\nconda install --channel=conda-forge --name=base conda-lock\n\n\nmamba install --channel=conda-forge --name=base conda-lock\n\n\n\nOnce conda-lock is installed, clim-recal can be installed via\n\npipxpipcondamamba\n\n\ncd `clim-recal`\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nconda activate base\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nmamba activate base\nconda-lock install conda-lock.yml",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html",
    "href": "R/misc/Identifying_Runs.html",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Ruth C E Bowyer 2023-06-13\nrm(list=ls())\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n\nScript to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus\n\n\n\nData is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\nY &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above\n\n\n\n\nThe result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06",
    "crumbs": [
      "Analysis in R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#about",
    "href": "R/misc/Identifying_Runs.html#about",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Script to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus",
    "crumbs": [
      "Analysis in R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#load-data",
    "href": "R/misc/Identifying_Runs.html#load-data",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Data is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;",
    "crumbs": [
      "Analysis in R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#comparing-runs",
    "href": "R/misc/Identifying_Runs.html#comparing-runs",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Y &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above",
    "crumbs": [
      "Analysis in R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#everything-combined",
    "href": "R/misc/Identifying_Runs.html#everything-combined",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "The result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06",
    "crumbs": [
      "Analysis in R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "docs/contributing.html",
    "href": "docs/contributing.html",
    "title": "Contributing to our project",
    "section": "",
    "text": "We welcome contributions to our repository and follow the Turing Way Contributing Guidelines. As the project develops we will expand this section with details more specific to our code base and potential use/application.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-quarto-locally",
    "href": "docs/contributing.html#running-quarto-locally",
    "title": "Contributing to our project",
    "section": "1.1 Running Quarto Locally",
    "text": "1.1 Running Quarto Locally\nIf you would like to render documentation locally you can do so via a conda or docker\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.\n\n1.1.1 Locally via conda\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\n\n\n1.1.2 Locally via docker\nIf you have docker installed, you can run a version of the jupyter conifiguration and quarto. The simplest and quickest solution, assuming you have docker running, is:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\nThis should generate local sessions of:\n\njupyter for the python/ model: http://localhost:8888\nquarto documentation: http://localhost:8080",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-conda",
    "href": "docs/contributing.html#running-tests-in-conda",
    "title": "Contributing to our project",
    "section": "4.1 Running tests in conda",
    "text": "4.1 Running tests in conda\nOnce the conda environment is installed, it should be straightforward to run the basic tests. The following example starts from a fresh clone of the repository:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ cd python         # Currently the tests must be run within the `python` folder\n$ pytest\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: code/clim-recal/python          # Path printed is tweaked here for convenience\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓sss✓✓✓✓✓✓✓✓sss✓                                  82% ████████▎\n utils.py ✓✓✓✓                                                              100% ██████████\nSaved badge to clim-recal/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 -----------\nName                                 Stmts   Miss  Cover\n--------------------------------------------------------\nconftest.py                             32      4    88%\ndata_download/ceda_ftp_download.py      59     59     0%\ndebiasing/preprocess_data.py           134    134     0%\ndebiasing/run_cmethods.py              108    108     0%\nload_data/data_loader.py                83     83     0%\nresampling/check_calendar.py            46     46     0%\nresampling/resampling_hads.py           59     59     0%\ntests/test_debiasing.py                188     27    86%\n--------------------------------------------------------\nTOTAL                                  732    520    29%\n\n5 files skipped due to complete coverage.\n\n=========================== short test summary info ============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n\nResults (0.14s):\n      16 passed\n       6 skipped\n       4 deselected\nThe SKIPPED messages of 6 doctests show they are automatically skipped if the linux server mount is not found, specifically data to test in /mnt/vmfileshare/ClimateData.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-docker",
    "href": "docs/contributing.html#running-tests-in-docker",
    "title": "Contributing to our project",
    "section": "4.2 Running tests in Docker",
    "text": "4.2 Running tests in Docker\nWith a docker install, tests can be run in two ways. The simplest is via docker compose:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\n$ docker compose exec jupyter bash -c \"conda run -n clim-recal --cwd python pytest\"\nThis mirrors the way tests are run via GitHub Actions for continuous integration on https://github.com/alan-turing-institute/clim-recal.\nTo run tests that require mounting ClimateData (which are not enabled by default), you will need to have a local mount of the relevant drive. This is easiest to achieve by building the compose/Dockerfile separately (not using compose) with that drive mounted.\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker build -f compose/Dockerfile --tag 'clim-recal-test' .\n$ docker run -it -p 8888:8888 -v /Volumes/vmfileshare:/mnt/vmfileshare clim-recal-test .\nThis will print information to the terminal including the link to the new jupyter session in this form:\n[I 2023-11-16 13:46:31.350 ServerApp]     http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url\nBy copying your equivalent of http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url you should be able to get a jupyer instance with all necessary packages installed running in your browser.\nFrom there, you can select a Terminal options under Other to get access to the terminal within your local docker build. You can then change to the python folder and run the tests with the server option to include ClimateData tests as well (note the a-hash-sequence will depend on your build):\n(clim-recal) jovyan@a-hash-sequence:~$ cd python\n(clim-recal) jovyan@a-hash-sequence:~/python$ pytest -m server\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: /home/jovyan/python\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓✓                         100% ██████████\nSaved badge to /home/jovyan/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 ---------\nName                                             Stmts   Miss  Cover\n--------------------------------------------------------------------\nconftest.py                                         32      4    88%\ndata_download/ceda_ftp_download.py                  59     59     0%\ndebiasing/preprocess_data.py                       134     21    84%\ndebiasing/python-cmethods/cmethods/CMethods.py     213    144    32%\ndebiasing/run_cmethods.py                          108      8    93%\nload_data/data_loader.py                            83     83     0%\nresampling/check_calendar.py                        46     46     0%\nresampling/resampling_hads.py                       59     59     0%\ntests/test_debiasing.py                            188      5    97%\nutils.py                                            23      5    78%\n--------------------------------------------------------------------\nTOTAL                                              945    434    54%\n\n5 files skipped due to complete coverage.\n\n\nResults (955.60s (0:15:55)):\n       4 passed\n      22 deselected",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "notebooks/cpm_projection_diff_plots.html",
    "href": "notebooks/cpm_projection_diff_plots.html",
    "title": "clim-recal",
    "section": "",
    "text": "import xarray\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.axes import Axes\nimport pandas as pd\n\nRUN_VARS = [\"01\", \"05\", \"06\", \"07\", \"08\"]\nVARS = [\"tasmax\", \"tasmin\", \"pr\"]\n\n\ndef get_root_filepath():\n    return \"https://climrecal.blob.core.windows.net/analysis/cpm-median-time-series\"\n\n\ndef add_cols(df):\n    df[\"day\"] = df[\"time\"].apply(lambda x: x.timetuple().tm_yday)\n    df[\"month\"] = df[\"time\"].apply(lambda x: x.month)\n    df[\"year\"] = df[\"time\"].apply(lambda x: x.year)\n    df[\"day_of_month\"] = df[\"time\"].apply(lambda x: x.day)\n    df[\"leap_year\"] = df[\"time\"].apply(lambda x: x.year % 4 == 0)\n    return df\n\n\ndef get_days(is_leap_year: bool) -&gt; np.array:\n    if not is_leap_year:\n        # https://docs.xarray.dev/en/stable/generated/xarray.Dataset.convert_calendar.html\n        #  February 6th (36), April 19th (109), July 2nd (183), September 12th (255), November 25th (329).\n        # First missing day should be 37, not 36 since February 6th is 37\n        return np.array([37, 109, 183, 255, 329])\n    else:\n        # January 31st (31), March 31st (91), June 1st (153), July 31st (213), September 31st (275) and November 30th (335).\n        return np.array([31, 91, 153, 213, 275, 335])\n\n\ndef get_data() -&gt; dict[str, dict[str, tuple[pd.DataFrame, pd.DataFrame]]]:\n    data = {}\n    for var in VARS:\n        data[var] = {}\n        for run_var in RUN_VARS:\n            path_raw = f\"{get_root_filepath()}/cpm-raw-medians/median-{var}-{run_var}.nc#mode=bytes\"\n            path_con = f\"{get_root_filepath()}/cpm-converted-nearest-medians/median-{var}-{run_var}.nc#mode=bytes\"\n            x_raw = xarray.load_dataset(path_raw)\n            x_con = xarray.load_dataset(path_con)\n            df_raw = add_cols(\n                x_raw.convert_calendar(\"standard\", align_on=\"year\")\n                .to_pandas()\n                .reset_index()\n            )\n            df_con = add_cols(x_con.to_pandas().reset_index())\n            data[var][run_var] = (df_raw, df_con)\n    return data\n\n\n# Load all data\ndata = get_data()\n\nWarning 3: Cannot find header.dxf (GDAL_DATA is not defined)\n\n\n\n# Example\nvar, run_var = \"tasmax\", \"01\"\ndf_raw, df_con = data[var][run_var]\n\n\n# Missing Day 37\ndf_raw.iloc[64:70]\n\n\n\n\n\n\n\n\ntime\ntasmax\nday\nmonth\nyear\nday_of_month\nleap_year\n\n\n\n\n64\n1981-02-04 12:00:00\n7.901025\n35\n2\n1981\n4\nFalse\n\n\n65\n1981-02-05 12:00:00\n9.846338\n36\n2\n1981\n5\nFalse\n\n\n66\n1981-02-07 12:00:00\n10.244776\n38\n2\n1981\n7\nFalse\n\n\n67\n1981-02-08 12:00:00\n7.123193\n39\n2\n1981\n8\nFalse\n\n\n68\n1981-02-09 12:00:00\n7.830713\n40\n2\n1981\n9\nFalse\n\n\n69\n1981-02-10 12:00:00\n7.916040\n41\n2\n1981\n10\nFalse\n\n\n\n\n\n\n\n\ndef plot_by_day(\n    data,\n    var: str,\n    run_var: str,\n    year: int,\n    leap_year: bool,\n    plot_diff: bool,\n    ax: Axes,\n    lw=0.8,\n):\n    df_raw, df_con = data[var][run_var]\n    if not plot_diff:\n        for i, df in enumerate([df_raw, df_con]):\n            if leap_year is not None:\n                df2 = df[df[\"leap_year\"].eq(leap_year)]\n            else:\n                df2 = df\n            if year is None:\n                x = df2.groupby(\"day\")[var].median()\n            else:\n                x = df2[df2[\"year\"].eq(year)].groupby(\"day\")[var].first()\n            if i == 0:\n                if leap_year is not None:\n                    ax.vlines(\n                        get_days(leap_year),\n                        x.min(),\n                        x.max(),\n                        zorder=-1,\n                        lw=0.4,\n                        color=\"k\",\n                        ls=\":\",\n                    )\n                ax.plot(x.index, x, lw=lw, alpha=0.8)\n            else:\n                ax.plot(x.index, x, lw=lw, alpha=0.8)\n                # pass\n    else:\n        series = []\n        for i, df in enumerate([df_raw, df_con]):\n            if leap_year is not None:\n                df2 = df[df[\"leap_year\"].eq(leap_year)]\n            else:\n                df2 = df\n            if year is None:\n                x = df2.groupby(\"day\")[var].median()\n            else:\n                x = df2[df2[\"year\"].eq(year)].groupby(\"day\")[var].first()\n            series.append(x)\n\n        # Proportion difference\n        # x2 = (series[1] - series[0]) / series[0]\n        # Use absolute difference\n        x2 = series[1] - series[0]\n\n        ax.vlines(\n            get_days(leap_year),\n            x2.min(),\n            x2.max(),\n            zorder=-1,\n            lw=0.4,\n            color=\"k\",\n            ls=\":\",\n        )\n        ax.plot(x2.index, x2, lw=lw, alpha=0.8)\n\n    ax.set_title(f\"Year: {year}, Leap year: {leap_year}\")\n    if not plot_diff:\n        ax.set_ylabel(f\"Absolute difference ({var})\")\n    else:\n        ax.set_ylabel(f\"Value ({var})\")\n\n\ndef plot_array(data, year, leap_year=True, plot_diff=False, lw=0.8):\n    run_vars = [\"01\", \"05\", \"06\", \"07\", \"08\"]\n    vars = [\"tasmax\", \"tasmin\", \"pr\"]\n    fig, axs = plt.subplots(\n        len(vars),\n        len(run_vars),\n        squeeze=False,\n        sharex=True,\n        sharey=\"row\",\n        figsize=(14, 6),\n    )\n    for row, var in enumerate(vars):\n        for col, run_var in enumerate(run_vars):\n            ax = axs[row][col]\n            plot_by_day(data, var, run_var, year, leap_year, plot_diff, ax=ax, lw=lw)\n            if col == 0:\n                ax.set_ylabel(var, fontsize=\"medium\")\n            else:\n                ax.set_ylabel(\"\")\n            if row == 0:\n                ax.set_title(f\"Run: {run_var}\", fontsize=\"medium\")\n            else:\n                ax.set_title(\"\")\n\n\nplot_by_day(data, \"tasmax\", \"08\", None, leap_year=True, plot_diff=False, ax=plt.gca())\n\n\n\n\n\n\n\n\n\nplot_by_day(data, \"tasmax\", \"08\", None, leap_year=True, plot_diff=True, ax=plt.gca())\n\n\n\n\n\n\n\n\n\nplot_by_day(data, \"tasmax\", \"08\", None, leap_year=False, plot_diff=True, ax=plt.gca())\n\n\n\n\n\n\n\n\n\n# Plot arrays\nfor year in [None, 1984]:\n    for leap_year in [True, False]:\n        for plot_diff in [False, True]:\n            if year is not None:\n                actual_year = year if leap_year else 1983\n            else:\n                actual_year = None\n            print(\n                f\"Plot diff: {plot_diff}; year: {actual_year}; leap_year: {leap_year}\"\n            )\n            # Leap years, medians\n            plot_array(data, year=actual_year, leap_year=leap_year, plot_diff=plot_diff)\n            plt.show()\n\nPlot diff: False; year: None; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: None; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: None; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: None; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: 1984; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: 1984; leap_year: True\n\n\n\n\n\n\n\n\n\nPlot diff: False; year: 1983; leap_year: False\n\n\n\n\n\n\n\n\n\nPlot diff: True; year: 1983; leap_year: False\n\n\n\n\n\n\n\n\n\n\n# Explore location of spike in difference in var=\"tasmax\", run_var=\"08\"\nplot_by_day(\n    data,\n    var=\"tasmax\",\n    run_var=\"08\",\n    year=1983,\n    leap_year=False,\n    plot_diff=False,\n    ax=plt.gca(),\n)\n# Subset days to region where spike is\nplt.xlim(320, 340)\n\n\n\n\n\n\n\n\n\nplot_by_day(\n    data,\n    var=\"tasmax\",\n    run_var=\"08\",\n    year=1983,\n    leap_year=False,\n    plot_diff=True,\n    ax=plt.gca(),\n)\nplt.xlim(320, 340)\n\n\n\n\n\n\n\n\n\n# Explore location of spike in difference in var=\"tasmax\", run_var=\"08\"\nplot_by_day(\n    data,\n    var=\"tasmax\",\n    run_var=\"08\",\n    year=1984,\n    leap_year=True,\n    plot_diff=False,\n    ax=plt.gca(),\n)\n# Subset days to region where spike is\nplt.xlim(320, 340)\n\n\n\n\n\n\n\n\n\n# Explore location of spike in difference in var=\"tasmax\", run_var=\"08\"\nplot_by_day(\n    data,\n    var=\"tasmax\",\n    run_var=\"08\",\n    year=1984,\n    leap_year=True,\n    plot_diff=True,\n    ax=plt.gca(),\n)\n# Subset days to region where spike is\nplt.xlim(320, 340)\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Appendix",
      "CPM Projection Analysis",
      "Temporal Interpolation Artefacts"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html",
    "href": "docs/reference/clim_recal.pipeline.html",
    "title": "1 clim_recal.pipeline",
    "section": "",
    "text": "clim_recal.pipeline\nWrappers to automate the entire pipeline.\nFollowing Andy’s very helpful excel file, this manages a reproduction of all steps of the debiasing pipeline.",
    "crumbs": [
      "API Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html#todo",
    "href": "docs/reference/clim_recal.pipeline.html#todo",
    "title": "1 clim_recal.pipeline",
    "section": "4.1 Todo",
    "text": "4.1 Todo\n\nUpdate the example here\nRemove out of date elements\nHardcode the process below\nRefactor to facilitate testing\nEnsure HADs still works\nAdd function for UKCP\nCheck convert_xr_calendar doctest examples\nFix order of UKCP changes\n\nTo run this step in the pipeline the following should work for the default combindations of variables: tasmax, tasmin, and rainfall and the default set of runs: 05, 06, 07 and 08, assuming the necessary data is mounted.\nIf installed via pipx/pip etc. on your local path (or within Docker) clim-recal should be a command line function\n$ clim-recal --all-variables --default-runs --output-path /where/results/should/be/written\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, regions_count=1, methods_count=1,\n                 cpm_folders_count=12, hads_folders_count=3, start_index=0,\n                 stop_index=None, cpus=2)&gt;\n&lt;CPMResamplerManager(variables_count=3, runs_count=4, input_paths_count=12)&gt;\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\nOtherwise, you can install locally and either run via pdm from the python folder\n$ cd clim-recal/python\n$ pdm run clim-recal --all-variables --default-runs --output-path /where/results/should/be/written\n# Skipping output for brevity\nOr within an ipython or jupyter instance (truncated below for brevity)\n&gt;&gt;&gt; from clim_recal.pipeline import main\n&gt;&gt;&gt; main(all_variables=True, default_runs=True)  # doctest: +SKIP\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, ...&gt;\nRegardless of your route, once you’re confident with the configuration, add the --execute parameter to run. For example, assuming a local install:\n$ clim-recal --all-variables --default-runs --output-path /where/results/should/be/written --execute\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, regions_count=1, methods_count=1,\n                 cpm_folders_count=12, hads_folders_count=3, start_index=0,\n                 stop_index=None, cpus=2)&gt;\n&lt;CPMResamplerManager(variables_count=3, runs_count=4, input_paths_count=12)&gt;\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\nRunning CPM Standard Calendar projection...\n&lt;CPMResampler(count=100, max_count=100,\n              input_path='/mnt/vmfileshare/ClimateData/Raw/UKCP2.2/tasmax/05/latest',\n              output_path='/mnt/vmfileshare/ClimateData/CPM-365/test-run-3-may/resample/\n              cpm/tasmax/05/latest')&gt;\n 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100  [ 0:38:27 &lt; 0:00:00 , 0 it/s ]\n  87% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/100  [ 0:17:42 &lt; 0:03:07 , 0 it/s ]",
    "crumbs": [
      "API Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html#functions",
    "href": "docs/reference/clim_recal.pipeline.html#functions",
    "title": "1 clim_recal.pipeline",
    "section": "8.1 Functions",
    "text": "8.1 Functions\n\n\n\nName\nDescription\n\n\n\n\nmain\nRun all elements of the pipeline.\n\n\n\n\n8.1.1 main\nclim_recal.pipeline.main(execute=False, hads_input_path=RAW_HADS_PATH, cpm_input_path=RAW_CPM_PATH, output_path=DEFAULT_OUTPUT_PATH, variables=(VariableOptions.default()), regions=(RegionOptions.default()), runs=(RunOptions.default()), methods=(MethodOptions.default()), all_variables=False, all_regions=False, default_runs=False, all_runs=False, all_methods=False, hads_projection=False, cpm_projection=False, crop_hads=True, crop_cpm=True, cpus=None, multiprocess=False, start_index=0, stop_index=None, total=None, print_range_length=5, **kwargs)\nRun all elements of the pipeline.\n\n8.1.1.1 Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nSequence[VariableOptions | str]\nVariables to include in the model, eg. tasmax, tasmin.\n(VariableOptions.default())\n\n\nruns\nSequence[RunOptions | str]\nWhich model runs to include, eg. “01”, “08”, “11”.\n(RunOptions.default())\n\n\nregions\nSequence[RegionOptions | str] | None\nWhich regions to crop data to. Future plans facilitate skipping to run for entire UK.\n(RegionOptions.default())\n\n\nmethods\nSequence[MethodOptions | str]\nWhich debiasing methods to apply.\n(MethodOptions.default())\n\n\noutput_path\nPathLike\nPath to save intermediate and final results to.\nDEFAULT_OUTPUT_PATH\n\n\ncpus\nint | None\nNumber of cpus to use when multiprocessing.\nNone\n\n\nmultiprocess\nbool\nWhether to use multiprocessing.\nFalse\n\n\nstart_index\nint\nIndex to start all iterations from.\n0\n\n\ntotal\nint | None\nTotal number of records to iterate over. 0 and None indicate all values from start_index.\nNone\n\n\n**kwargs\n\nAdditional parameters to pass to a ClimRecalConfig.\n{}\n\n\n\n\n\n8.1.1.2 Notes\nThe default parameters here are meant to reflect the entire workflow process to ease reproducibility.\n\n\n8.1.1.3 Examples\nNote the _allow_check_fail parameters support running the examples without data mounted from a server.\n&gt;&gt;&gt; main(variables=(\"rainfall\", \"tasmin\"),\n...      output_path=test_runs_output_path,\n...      cpm_kwargs=dict(_allow_check_fail=True),\n...      hads_kwargs=dict(_allow_check_fail=True),\n... )\n'set_cpm_for_coord_alignment' for 'HADs' not speficied.\nDefaulting to 'self.cpm_input_path': '...'\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=2, runs_count=1,\n                 regions_count=1, methods_count=1,\n                 cpm_folders_count=2, hads_folders_count=2,\n                 start_index=0, stop_index=None,\n                 cpus=...)&gt;\n&lt;CPMResamplerManager(variables_count=2, runs_count=1,\n                     input_paths_count=2)&gt;\n&lt;HADsResamplerManager(variables_count=2, input_paths_count=2)&gt;\nNo steps run. Add '--execute' to run steps.",
    "crumbs": [
      "API Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/docker-configurations.html",
    "href": "docs/docker-configurations.html",
    "title": "Docker configurations",
    "section": "",
    "text": "The compose.yml file and compose/ folder provider Docker configurations for deploying and testing this code and documentation reproducibly. These require an internet connection and Docker installation.",
    "crumbs": [
      "Appendix",
      "Docker"
    ]
  },
  {
    "objectID": "docs/docker-configurations.html#adding-multiple-users",
    "href": "docs/docker-configurations.html#adding-multiple-users",
    "title": "Docker configurations",
    "section": "3.1 Adding multiple users",
    "text": "3.1 Adding multiple users\nSome utility functions are provided to ease adding many users for a workshop. We do not recommend using this for any long term deployment, and certainly not for any data or environments with security concerns.\n\n\n\n\n\n\nWarning\n\n\n\nThese examples are not tested for security. Rather: this is intended for a short workshop that can scale a series of environements generated via docker for many users to have remote access to play with the code and data provided.\n\n\nOur usecase is primarily for within a docker deploy with root permission.\nA list of user names and passwords are needed, and by default we assume those are in a table format in the following structure:\n\n\n\nuser_name\npassword\n\n\n\n\nsally\nfig*new£kid\n\n\ngeorge\ntree&iguana*sky\n\n\nsusan\nhistory!bill-walk\n\n\n\nwhich in csv would look like:\nuser_name,password\nsally,fig*new£kid\ngeorge,tee&iguana*sky\nsusan,history!bill-walk\nTo generate basic user accounts one could run the following with root permission in an rstudio or jupyter environment (assuming rstudio has been built):\ndocker compose up -d jupyter\n[+] Running 1/1\n ✔ Container clim-recal-jupyter-1  Started\ndocker compose -u 0 exec jupyter bash\n(base) root@aha22hnum:~#\nwhich should instantiate a bash terminal in the Docker environment for the default jupyter Docker root user. The following demonstrates creating a csv file config in test_auth.csv:\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt;\n&gt;&gt;&gt; csv_path: Path = 'test_auth.csv'\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\nand then with that file generate users and their home folders from test_auth.csv:\n&gt;&gt;&gt; from utils.server import JUPYTER_DOCKER_USER_PATH, csv_reader, make_users,\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=csv_path,\n...     user_col=\"user_name\",\n...     password_col=\"password\",\n...     file_reader=csv_reader,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; tuple(user_paths)\n('/home/sally', '/home/george', '/home/susan')\n&gt;&gt;&gt; csv_path.unlink()",
    "crumbs": [
      "Appendix",
      "Docker"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html",
    "href": "docs/deprecated_pipeline.html",
    "title": "Analysis pipeline guidance",
    "section": "",
    "text": "This is a deprecated guide to our analysis pipeline.",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html#prerequisites",
    "href": "docs/deprecated_pipeline.html#prerequisites",
    "title": "Analysis pipeline guidance",
    "section": "1 Prerequisites",
    "text": "1 Prerequisites\nFor our bias correction methods, we tap into dedicated packages in both python and R ecosystems. The integration of these languages allows us to utilize the cutting-edge functionalities implemented in each.\nGiven this dual-language nature of our analysis pipeline, we also provide preprocessing scripts written in both python and R. To facilitate a seamless experience, users are required to set up both python and R environments as detailed below.\n\n1.1 Setting up your R environment\n\n1.1.1 Download and Install R\nVisit CRAN (The Comprehensive R Archive Network) to download the latest version of R compatible with your operating system. Then verify successful installation via command line:\nR --version\n\n\n1.1.2 Install Necessary R Packages\nOur analysis utilizes several R packages. You can install these packages by starting R (just type R in your command line and press enter) and entering the following commands in the R console:\ninstall.packages(\"package1\")\ninstall.packages(\"package2\")\n#... (continue for all necessary packages)\nReplace \"package1\", \"package2\", etc., with the actual names of the required packages. A list of required R packages is usually at the top of each .R, .Rmd, some R specific .md files. Below is an example from WIP Comparing HADs grids:\nlibrary(terra)\nlibrary(sp)\nlibrary(exactextractr)\n\n\n\n1.2 Setting up your python environment\nFor your python environment, we provide a conda-lock.yml environment file for ease-of-use.\nconda-lock install -name clim-recal conda-lock.yml\n\nIn order to paralellize the reprojection step, we make use of the GNU parallel shell tool.\nparallel --version\n\n\n1.3 The cmethods library\nThis repository contains a python script used to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\ngit submodule update --init --recursive\n\n\n1.4 Downloading the data\n\n1.4.1 Climate data\nThis streamlined pipeline is designed for raw data provided by the Met Office, accessible through the CEDA archive. It utilizes UKCP control, scenario data at 2.2km resolution, and HADs observational data.\nTo access the data, register here at the CEDA archive and configure your FTP credentials in “My Account”. Utilize our ceda_ftp_download.py script to download the data.\n# cpm data\npython3 ceda_ftp_download.py --input /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85/ --output 'output_dir' --username 'uuu' --psw 'ppp' --change_hierarchy\n\n# hads data\npython3 ceda_ftp_download.py --input /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km --output 'output_dir' --username 'uuu' --psw 'ppp'\nYou need to replace uuu and ppp with your CEDA username and FTP password respectively and replace output_dir with the directory you want to write the data to.\nThe --change_hierarchy flag modifies the folder hierarchy to fit with the hierarchy in the Turing Azure file store. This flag only applies to the UKCP data and should not be used with HADs data. You can use the same script without the --change_hierarchy flag in order to download files without any changes to the hierarchy.\n\n\n1.4.2 Geospatial data\nIn addition to the climate data we use geospatial data to divide the data into smaller chunks. Specifically we use the following datasets for city boundaries:\n\nScottish localities boundaries for cropping out Glasgow. Downloaded from nrscotland.gov.uk on 1st Aug 2023\nMajor Towns and Cities boundaries for cropping out Manchester. Downloaded from https://geoportal.statistics.gov.uk/",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html#reprojecting-the-data",
    "href": "docs/deprecated_pipeline.html#reprojecting-the-data",
    "title": "Analysis pipeline guidance",
    "section": "2 Reprojecting the data",
    "text": "2 Reprojecting the data\nThe HADs data and the UKCP projections have different resolution and coordinate system. For example: the HADs dataset uses the British National Grid coordinate system.\n\n\nClick for raw HADs and CPM coordinate structures\n\nCRS.from_wkt('PROJCS[\"undefined\",GEOGCS[\"undefined\",DATUM[\"undefined\",SPHEROID[\"undefined\",6377563.396,299.324961266495]],PRIMEM[\"undefined\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",49],PARAMETER[\"central_meridian\",-2],PARAMETER[\"scale_factor\",0.9996012717],PARAMETER[\"false_easting\",400000],PARAMETER[\"false_northing\",-100000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]')\nCRS.from_wkt('GEOGCRS[\"undefined\",BASEGEOGCRS[\"undefined\",DATUM[\"undefined\",ELLIPSOID[\"undefined\",6371229,0,LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]],PRIMEM[\"undefined\",0,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]],DERIVINGCONVERSION[\"Pole rotation (netCDF CF convention)\",METHOD[\"Pole rotation (netCDF CF convention)\"],PARAMETER[\"Grid north pole latitude (netCDF CF convention)\",37.5,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],PARAMETER[\"Grid north pole longitude (netCDF CF convention)\",177.5,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],PARAMETER[\"North pole grid longitude (netCDF CF convention)\",0,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]],CS[ellipsoidal,2],AXIS[\"longitude\",east,ORDER[1],ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],AXIS[\"latitude\",north,ORDER[2],ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]]')\n\nThe first step in our analysis pipeline is to reproject the UKCP datasets to the British National Grid coordinate system. For this purpose, we utilize the Geospatial Data Abstraction Library (GDAL), designed for reading and writing raster and vector geospatial data formats.\n\nNote that, to reproduce our exact pipeline, we switch environments here as explained in the requirements.\nconda activate gdal_env\n\nTo execute the reprojection in parallel fashion, run the reproject_all.sh script from your shell. As an input to the script replace path_to_netcdf_files with the path to the raw netCDF files.\ncd bash\nsh reproject_all.sh path_to_netcdf_files",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html#resampling-the-data",
    "href": "docs/deprecated_pipeline.html#resampling-the-data",
    "title": "Analysis pipeline guidance",
    "section": "3 Resampling the data",
    "text": "3 Resampling the data\nResample the HADs UK dataset from 1km to 2.2km grid to match the UKCP reprojected grid. We run the resampling python script specifying the --input location of the reprojected files from the previous step, the UKCP --grid file an the --output location for saving the resampled files.\n# switch to main environment\nconda activate clim-recal\n\n# run resampling\ncd ../python/resampling\npython resampling_hads.py --input path_to_reprojected --grid path_to_grid_file --output path_to_resampled",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html#preparing-the-bias-correction-and-assessment",
    "href": "docs/deprecated_pipeline.html#preparing-the-bias-correction-and-assessment",
    "title": "Analysis pipeline guidance",
    "section": "4 Preparing the bias correction and assessment",
    "text": "4 Preparing the bias correction and assessment\n\n4.1 Spatial cropping\nBecause the bias correction process is computationally intensive, handling large datasets can be challenging and time-consuming. Therefore, to make the pipeline more manageable and efficient, it is recommended to split the data into smaller subsets.\nFor the purposes of our example pipeline, we’ve opted for reducing the data to individual city boundaries. To crop you need to adjust the paths in Cropping_Rasters_to_three_cities.R script to fit 1your own directory sturcture. The cropping is implemented in the cpm_read_crop and hads_read_crop functions.\nRscript Cropping_Rasters_to_three_cities.R\n\n\n4.2 calibration-validation data split\nFor the purpose of assessing our bias correction, we then split our data, both the projection as well as the ground-truth observations by dates. In this example here we calibrate the bias correction based on the years 1981 to 1983.\nWe then use data from year 2010 to validate the quality of the bias correction. You need to replace path_to_cropped with the path where the data from the previous cropping step was saved and path_to_preprocessed with the output directory you choose. You can leave the -v and -r flags as specified below or choose another metric and run if you prefer.\ncd ../debiasing\npython preprocess_data.py --mod path_to_cropped --obs path_to_cropped -v tasmax -r '05' --out path_to_preprocessed --calib_dates 19810101-19831230 --valid_dates 20100101-20101230\nThe preprocess_data.py script also aligns the calendars of the historical simulation data and observed data, ensuring that they have the same time dimension and checks that the observed and simulated historical data have the same dimensions.\n\n\n\n\n\n\nNote\n\n\n\npreprocess_data.py makes use of our custom data loader functions. In data_loader.py we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb.",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/deprecated_pipeline.html#applying-the-bias-correction",
    "href": "docs/deprecated_pipeline.html#applying-the-bias-correction",
    "title": "Analysis pipeline guidance",
    "section": "5 Applying the bias correction",
    "text": "5 Applying the bias correction\n\n\n\n\n\n\nNote\n\n\n\nBy March 2023 we have only implemented the python-cmethods library.\n\n\nThe run_cmethods.py allow us to adjusts climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data), control data (historical UKCP data), and scenario data (future UKCP data), and applies a correction method to the scenario data. The resulting output is saved as a .nc to a specified directory.\nThe script will also produce a time-series and a map plot of the debiased data. To run this you need to replace path_to_validation_data with the output directories of the previous step and specify path_to_corrected_data as your output directory for the bias corrected data. You can also specify your preferred bias_correction_method (e.g. quantile_delta_mapping).\npython3 run_cmethods.py --input_data_folder path_to_validation_data --out path_to_corrected_data --method bias_correction_method --v 'tas'\nThe run_cmethods.py script loops over the time periods and applies debiasing in periods of 10 years in the following steps:\n\nLoads the scenario data for the current time period.\nApplies the specified correction method to the scenario data.\nSaves the resulting output to the specified directory.\nCreates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\n\nFor each 10 year time period it will produce an .nc output file with the adjusted data and a time-series plot and a time dependent map plot of the adjusted data.",
    "crumbs": [
      "Appendix",
      "Deprecated"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.data_loader.html",
    "href": "docs/reference/clim_recal.data_loader.html",
    "title": "1 clim_recal.data_loader",
    "section": "",
    "text": "clim_recal.data_loader\n\n\n\n\n\nName\nDescription\n\n\n\n\nclip_dataset\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\nload_and_merge\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\nload_data\nThis function takes a date range and a variable and loads and merges\n\n\nreformat_file\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\nclim_recal.data_loader.clip_dataset(xa, variable, shapefile)\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxa\nDataArray\nxArray containing a given variable (e.g. rainfall)\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nshapefile\nstr\nPath to a shape file used to clip resulting dataset, must be in the same CRS of the input xArray.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA clipped xarray dataset\n\n\n\n\n\n\n\n\n\n\nclim_recal.data_loader.load_and_merge(date_range, files, variable, write_crs_format=BRITISH_NATIONAL_GRID_EPSG)\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_range\nDateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nfiles\nlist[str]\nList of strings with path to files to be loaded.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nAn xarray containing all loaded and merged data\n\n\n\n\n\n\n\nclim_recal.data_loader.load_data(input_path, date_range, variable, filter_filenames_on_variable=False, run_number=None, filter_filenames_on_run_number=False, use_pr=False, shapefile_path=None, extension='nc')\nThis function takes a date range and a variable and loads and merges xarrays based on those parameters.\nIf shapefile is provided it crops the data to that region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_path\nstr\nPath to where .nc or .tif files are found\nrequired\n\n\ndate_range\nDateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nfilter_filenames_on_variable\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “variable” as a substring. When False, filtering does not happen.\nFalse\n\n\nrun_number\nstr | None\nA string representing the CPM run number to use (out of 13 CPM runs available in the database). Only files whose file name contains the substring run_number will be used. If None, all files in input_path are parsed, regardless of run number in filename.\nNone\n\n\nfilter_filenames_on_run_number\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “2.2km_” followed by “run_number”. When False, filtering does not happen. This should only be used for CPM files. For HADs files this should always be set to False.\nFalse\n\n\nuse_pr\nbool\nIf True, replace variable with “pr” string when filtering the file names.\nFalse\n\n\nshapefile_path\nstr | None\nPath to a shape file used to clip resulting dataset.\nNone\n\n\nextension\nstr\nExtension of the files to be loaded, it can be .nc or .tif files.\n'nc'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA DataArray containing all loaded and merged and clipped data\n\n\n\n\n\n\n\nclim_recal.data_loader.reformat_file(file, variable, spatial_config=BRITISH_NATIONAL_GRID_EPSG)\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile\nstr\nPath as a str to tiff file.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded.\nrequired\n\n\nspatial_config\nstr\nSpatial coordinate configuration for export.\nBRITISH_NATIONAL_GRID_EPSG\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA formatted xarray",
    "crumbs": [
      "API Reference",
      "Data Loading"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.data_loader.html#functions",
    "href": "docs/reference/clim_recal.data_loader.html#functions",
    "title": "1 clim_recal.data_loader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nclip_dataset\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\nload_and_merge\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\nload_data\nThis function takes a date range and a variable and loads and merges\n\n\nreformat_file\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\nclim_recal.data_loader.clip_dataset(xa, variable, shapefile)\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxa\nDataArray\nxArray containing a given variable (e.g. rainfall)\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nshapefile\nstr\nPath to a shape file used to clip resulting dataset, must be in the same CRS of the input xArray.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA clipped xarray dataset\n\n\n\n\n\n\n\n\n\n\nclim_recal.data_loader.load_and_merge(date_range, files, variable, write_crs_format=BRITISH_NATIONAL_GRID_EPSG)\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_range\nDateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nfiles\nlist[str]\nList of strings with path to files to be loaded.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nAn xarray containing all loaded and merged data\n\n\n\n\n\n\n\nclim_recal.data_loader.load_data(input_path, date_range, variable, filter_filenames_on_variable=False, run_number=None, filter_filenames_on_run_number=False, use_pr=False, shapefile_path=None, extension='nc')\nThis function takes a date range and a variable and loads and merges xarrays based on those parameters.\nIf shapefile is provided it crops the data to that region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_path\nstr\nPath to where .nc or .tif files are found\nrequired\n\n\ndate_range\nDateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nfilter_filenames_on_variable\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “variable” as a substring. When False, filtering does not happen.\nFalse\n\n\nrun_number\nstr | None\nA string representing the CPM run number to use (out of 13 CPM runs available in the database). Only files whose file name contains the substring run_number will be used. If None, all files in input_path are parsed, regardless of run number in filename.\nNone\n\n\nfilter_filenames_on_run_number\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “2.2km_” followed by “run_number”. When False, filtering does not happen. This should only be used for CPM files. For HADs files this should always be set to False.\nFalse\n\n\nuse_pr\nbool\nIf True, replace variable with “pr” string when filtering the file names.\nFalse\n\n\nshapefile_path\nstr | None\nPath to a shape file used to clip resulting dataset.\nNone\n\n\nextension\nstr\nExtension of the files to be loaded, it can be .nc or .tif files.\n'nc'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA DataArray containing all loaded and merged and clipped data\n\n\n\n\n\n\n\nclim_recal.data_loader.reformat_file(file, variable, spatial_config=BRITISH_NATIONAL_GRID_EPSG)\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile\nstr\nPath as a str to tiff file.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded.\nrequired\n\n\nspatial_config\nstr\nSpatial coordinate configuration for export.\nBRITISH_NATIONAL_GRID_EPSG\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataArray\nA formatted xarray",
    "crumbs": [
      "API Reference",
      "Data Loading"
    ]
  },
  {
    "objectID": "docs/cpm_projection.html",
    "href": "docs/cpm_projection.html",
    "title": "UKCP 2.2 Temporal Interpolation",
    "section": "",
    "text": "To align climate projections with measured records, we interpolate five or six extra time points—standard or leap years respectively—per year.\n\n\nCode\nfrom typing import Final\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom xarray.core.types import T_Dataset\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\nfrom clim_recal.utils.data import VariableOptions, RunOptions\nfrom clim_recal.utils.xarray import (\n    annual_group_xr_time_series,\n    plot_xarray,\n)\nfrom clim_recal.utils.docs import CPMSummaryTimeSeries, gap_360_days, plot_axvlines\n\nvars: tuple[str] = ('tasmax', 'tasmin', 'pr')\nruns: tuple[str] = ('01', '05', '06', '07', '08')\n\ncpm_summary_ts = CPMSummaryTimeSeries(runs=runs, variables=vars)\n\ncpm_medians: dict[str, dict[tuple[str, str], T_Dataset]] = cpm_summary_ts.get_local_xarrays_dict()\n\ncal_360_day_4_years: Final[int] = 360*4\ncal_standard_4_years: Final[int] = 365*3 + 366\n\ncheck_days: Final[int] = 6\n\n\nWarning 3: Cannot find header.dxf (GDAL_DATA is not defined)\n\n\nWith the CPM annual timeseries loaded, we can now plot projections from 1 December 1980 to 30 November 2080:\n\n\nCode\nfig, axs = plt.subplots(3, sharex=True)\nfor run_id, cpm_raw_median in cpm_medians['raw'].items():\n    var, run = run_id\n    cpm_raw_median[var][:cal_360_day_4_years*2].plot(\n        label=f\"run-{run}\",\n        ax=axs[vars.index(var)],\n    )\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Raw CPM projection 360 day years\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(3, sharex=True)\nfor run_id, cpm_convert_linear_median in cpm_medians['linear'].items():\n    var, run = run_id\n    cpm_convert_linear_median[var][:cal_standard_4_years*2].plot(\n        label=f\"run-{run}\",\n        ax=axs[vars.index(var)],\n    )\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Converted CPM to standard years\n\n\n\n\n\n\n\nCode\nvar = 'tasmax'\nrun = '01'\nrun_id = (var, run)\ncpm_convert_linear_median = cpm_medians['linear'][run_id]\ntrimmed_raw_to_standard = (\n    cpm_medians['raw'][run_id][var][:check_days].convert_calendar('standard', align_on=\"year\")\n)\n\ntrimmed_raw_to_standard.plot(label=\"raw\", marker=\".\")\ncpm_convert_linear_median[var][:check_days + 1].plot(label=\"convert\", marker=\".\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Compare CPM tasmax run 01 360 vs projected standard leap year December\n\n\n\n\n\n\n\nCode\ncheck_days: Final[int] = 6\n\nfig, axs = plt.subplots(3, 5, sharex=True, sharey=True)\nfor run_id, cpm_convert_linear_median in cpm_medians['linear'].items():\n    var, run = run_id\n    trimmed_raw_to_standard = (\n        cpm_medians['raw'][run_id][var][:check_days].convert_calendar(\n            'standard', align_on=\"year\"\n        )\n    )\n    cpm_convert_linear_median[var][:check_days + 1].plot(\n        label=\"convert\",\n        marker=\".\",\n        ax=axs[vars.index(var), runs.index(run)]\n      )\n    trimmed_raw_to_standard.plot(\n        label=\"raw\",\n        marker=\".\",\n        ax=axs[vars.index(var), runs.index(run)],\n      )\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Compare CPM 360 vs projected standard leap year December\n\n\n\n\n\nAverage annual projections.\n\n\nCode\nannual_raw_medians: T_Dataset = annual_group_xr_time_series(\n  cpm_medians['raw']['tasmax', '01'].tasmax,\n  variable_name=VariableOptions.TASMAX,\n  plot_path=None,\n  time_stamp=None)\n\nannual_convert_linear_medians: T_Dataset = annual_group_xr_time_series(\n  cpm_medians['linear']['tasmax', '01'].tasmax,\n  variable_name=VariableOptions.TASMAX,\n  plot_path=None,\n  time_stamp=None)\n\nannual_convert_nearest_medians: T_Dataset = annual_group_xr_time_series(\n  cpm_medians['nearest']['tasmax', '01'].tasmax,\n  variable_name=VariableOptions.TASMAX,\n  plot_path=None,\n  time_stamp=None)\n\nannual_raw_medians.plot(label='raw', linewidth=5)\nannual_convert_linear_medians.plot(label='convert', linewidth=3)\nannual_convert_nearest_medians.plot(label='nearest', linewidth=1)\n\nplot_axvlines(plt, gap_360_days(is_leap_year=False))\n\nplt.title(\"Medians of taxmax for each day of the year 1980-01-01 to 2080-11-30\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Annual average of means of raw tasmax CPM 360 day years\n\n\n\n\n\nFor more detailed analysis comparing nearest and linear interpolation see:\n\ncpm_projection_diff_plots.ipynb\ncpm_projection_diff_plots_linear_nearest.ipynb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Appendix",
      "CPM Projection Analysis"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.data.html",
    "href": "docs/reference/clim_recal.utils.data.html",
    "title": "1 clim_recal.utils.data",
    "section": "",
    "text": "clim_recal.utils.data\n\n\n\n\n\nName\nDescription\n\n\n\n\nBoundsTupleType\nGeoPandas bounds: (minx, miny, maxx, maxy).\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.\n\n\nGlasgowCoordsEPSG27700\nGlasgow box coordinates in 27700 grid.\n\n\nLondonCoordsEPSG27700\nLondon box coordinates in 27700 grid.\n\n\nMONTH_DAY_XARRAY_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 leap years.\n\n\nMONTH_DAY_XARRAY_NO_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 non leap years.\n\n\nManchesterCoordsEPSG27700\nManchester box coordinates in 27700 grid.\n\n\nScotlandCoordsEPSG27700\nScotland box coordinates in 27700 grid.\n\n\nTHREE_CITY_CENTRE_COORDS\nCity centre (lon, lat) tuple coords of Glasgow, Manchester and London.\n\n\nXArrayEngineType\nEngine types supported by xarray as str.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nBoundingBoxCoords\nA region name and its bounding box coordinates.\n\n\nDataLicense\nClass for standardising data license references.\n\n\nMetaData\nManage info on source material.\n\n\nMethodOptions\nSupported options for methods.\n\n\nRegionOptions\nSupported options for variables.\n\n\nRunOptions\nSupported options for variables.\n\n\nVariableOptions\nSupported variables options and related configuration.\n\n\n\n\n\nclim_recal.utils.data.BoundingBoxCoords(self, name, xmin, xmax, ymin, ymax, epsg=BRITISH_NATION_GRID_COORDS_NUMBER)\nA region name and its bounding box coordinates.\n\n\n\n\n\nName\nDescription\n\n\n\n\nrioxarry_epsg\nReturn self.epsg in rioxarray str format.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_rioxarray_dict\nReturn coords as dict\n\n\nas_rioxarray_tuple\nReturn in xmin, xmax, ymin, ymax order.\n\n\n\n\n\nclim_recal.utils.data.BoundingBoxCoords.as_rioxarray_dict()\nReturn coords as dict\n\n\n\nclim_recal.utils.data.BoundingBoxCoords.as_rioxarray_tuple()\nReturn in xmin, xmax, ymin, ymax order.\n\n\n\n\n\nclim_recal.utils.data.DataLicense(self, name, url, version)\nClass for standardising data license references.\n\n\n\nclim_recal.utils.data.MetaData(self, name, slug, region=None, description=None, date_created=None, authors=None, url=None, info_url=None, doi=None, path=None, license=lambda: OpenGovernmentLicense(), dates=None, date_published=None, unit=None, cite_as=None)\nManage info on source material.\n\n\n\nclim_recal.utils.data.MethodOptions()\nSupported options for methods.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.MethodOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.MethodOptions.default()\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.RegionOptions()\nSupported options for variables.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\nbounding_box\ndict for accessing bounding boxes of included Regions.\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.RegionOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RegionOptions.bounding_box(region_name)\ndict for accessing bounding boxes of included Regions.\n\n\n\nclim_recal.utils.data.RegionOptions.default()\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.RunOptions()\nSupported options for variables.\n\n\nOptions TWO and THREE are not available for UKCP2.2.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\npreferred\nReturn the preferred runs determined by initial results.\n\n\npreferred_and_first\nReturn the preferred and first runs.\n\n\n\n\n\nclim_recal.utils.data.RunOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RunOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.RunOptions.preferred()\nReturn the preferred runs determined by initial results.\n\n\nSee R/misc/Identifying_Runs.md for motivation and results.\n\n\n\n\nclim_recal.utils.data.RunOptions.preferred_and_first()\nReturn the preferred and first runs.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions()\nSupported variables options and related configuration.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ncpm_value\nReturn CPM value equivalent of variable.\n\n\ncpm_values\nReturn CPM values equivalent of variable.\n\n\ndefault\nDefault option.\n\n\ndefault_resample_method\nDefault resampling method.\n\n\nresampling_method\nReturn resampling method for variable.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_value(variable)\nReturn CPM value equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nVariableOptions attribute to query value of.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_value('rainfall')\n'pr'\n&gt;&gt;&gt; VariableOptions.cpm_value('tasmin')\n'tasmin'\n\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_values(variables=None)\nReturn CPM values equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nIterable[str] | None\nVariableOptions attributes to query values of.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_values(['rainfall', 'tasmin'])\n('pr', 'tasmin')\n\n\n\n\nclim_recal.utils.data.VariableOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.VariableOptions.default_resample_method()\nDefault resampling method.\n\n\n\nclim_recal.utils.data.VariableOptions.resampling_method(variable)\nReturn resampling method for variable.\nFor details see: https://rasterio.readthedocs.io/en/stable/api/rasterio.enums.html#rasterio.enums.Resampling\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr | None\nVariableOptions attribute to query resampling method from.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValue to access related resampling method.\n\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.resampling_method('rainfall')\n&lt;Resampling.average: 5&gt;\n&gt;&gt;&gt; VariableOptions.resampling_method('tasmin')\n&lt;Resampling.min: 9&gt;\n&gt;&gt;&gt; VariableOptions.resampling_method(None)\n&lt;Resampling.average: 5&gt;",
    "crumbs": [
      "API Reference",
      "Utilities",
      "data"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.data.html#attributes",
    "href": "docs/reference/clim_recal.utils.data.html#attributes",
    "title": "1 clim_recal.utils.data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBoundsTupleType\nGeoPandas bounds: (minx, miny, maxx, maxy).\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.\n\n\nGlasgowCoordsEPSG27700\nGlasgow box coordinates in 27700 grid.\n\n\nLondonCoordsEPSG27700\nLondon box coordinates in 27700 grid.\n\n\nMONTH_DAY_XARRAY_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 leap years.\n\n\nMONTH_DAY_XARRAY_NO_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 non leap years.\n\n\nManchesterCoordsEPSG27700\nManchester box coordinates in 27700 grid.\n\n\nScotlandCoordsEPSG27700\nScotland box coordinates in 27700 grid.\n\n\nTHREE_CITY_CENTRE_COORDS\nCity centre (lon, lat) tuple coords of Glasgow, Manchester and London.\n\n\nXArrayEngineType\nEngine types supported by xarray as str.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "data"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.data.html#classes",
    "href": "docs/reference/clim_recal.utils.data.html#classes",
    "title": "1 clim_recal.utils.data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBoundingBoxCoords\nA region name and its bounding box coordinates.\n\n\nDataLicense\nClass for standardising data license references.\n\n\nMetaData\nManage info on source material.\n\n\nMethodOptions\nSupported options for methods.\n\n\nRegionOptions\nSupported options for variables.\n\n\nRunOptions\nSupported options for variables.\n\n\nVariableOptions\nSupported variables options and related configuration.\n\n\n\n\n\nclim_recal.utils.data.BoundingBoxCoords(self, name, xmin, xmax, ymin, ymax, epsg=BRITISH_NATION_GRID_COORDS_NUMBER)\nA region name and its bounding box coordinates.\n\n\n\n\n\nName\nDescription\n\n\n\n\nrioxarry_epsg\nReturn self.epsg in rioxarray str format.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nas_rioxarray_dict\nReturn coords as dict\n\n\nas_rioxarray_tuple\nReturn in xmin, xmax, ymin, ymax order.\n\n\n\n\n\nclim_recal.utils.data.BoundingBoxCoords.as_rioxarray_dict()\nReturn coords as dict\n\n\n\nclim_recal.utils.data.BoundingBoxCoords.as_rioxarray_tuple()\nReturn in xmin, xmax, ymin, ymax order.\n\n\n\n\n\nclim_recal.utils.data.DataLicense(self, name, url, version)\nClass for standardising data license references.\n\n\n\nclim_recal.utils.data.MetaData(self, name, slug, region=None, description=None, date_created=None, authors=None, url=None, info_url=None, doi=None, path=None, license=lambda: OpenGovernmentLicense(), dates=None, date_published=None, unit=None, cite_as=None)\nManage info on source material.\n\n\n\nclim_recal.utils.data.MethodOptions()\nSupported options for methods.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.MethodOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.MethodOptions.default()\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.RegionOptions()\nSupported options for variables.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\nbounding_box\ndict for accessing bounding boxes of included Regions.\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.RegionOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RegionOptions.bounding_box(region_name)\ndict for accessing bounding boxes of included Regions.\n\n\n\nclim_recal.utils.data.RegionOptions.default()\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.RunOptions()\nSupported options for variables.\n\n\nOptions TWO and THREE are not available for UKCP2.2.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\npreferred\nReturn the preferred runs determined by initial results.\n\n\npreferred_and_first\nReturn the preferred and first runs.\n\n\n\n\n\nclim_recal.utils.data.RunOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RunOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.RunOptions.preferred()\nReturn the preferred runs determined by initial results.\n\n\nSee R/misc/Identifying_Runs.md for motivation and results.\n\n\n\n\nclim_recal.utils.data.RunOptions.preferred_and_first()\nReturn the preferred and first runs.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions()\nSupported variables options and related configuration.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ncpm_value\nReturn CPM value equivalent of variable.\n\n\ncpm_values\nReturn CPM values equivalent of variable.\n\n\ndefault\nDefault option.\n\n\ndefault_resample_method\nDefault resampling method.\n\n\nresampling_method\nReturn resampling method for variable.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_value(variable)\nReturn CPM value equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nVariableOptions attribute to query value of.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_value('rainfall')\n'pr'\n&gt;&gt;&gt; VariableOptions.cpm_value('tasmin')\n'tasmin'\n\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_values(variables=None)\nReturn CPM values equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nIterable[str] | None\nVariableOptions attributes to query values of.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_values(['rainfall', 'tasmin'])\n('pr', 'tasmin')\n\n\n\n\nclim_recal.utils.data.VariableOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.VariableOptions.default_resample_method()\nDefault resampling method.\n\n\n\nclim_recal.utils.data.VariableOptions.resampling_method(variable)\nReturn resampling method for variable.\nFor details see: https://rasterio.readthedocs.io/en/stable/api/rasterio.enums.html#rasterio.enums.Resampling\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr | None\nVariableOptions attribute to query resampling method from.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValue to access related resampling method.\n\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.resampling_method('rainfall')\n&lt;Resampling.average: 5&gt;\n&gt;&gt;&gt; VariableOptions.resampling_method('tasmin')\n&lt;Resampling.min: 9&gt;\n&gt;&gt;&gt; VariableOptions.resampling_method(None)\n&lt;Resampling.average: 5&gt;",
    "crumbs": [
      "API Reference",
      "Utilities",
      "data"
    ]
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded and processed\n\n\n\nclim_recal.pipeline\nWrappers to automate the entire pipeline.\n\n\nclim_recal.ceda_ftp_download\n\n\n\nclim_recal.data_loader\n\n\n\nclim_recal.config\n\n\n\nclim_recal.resample\nResample UKHADS data and UKCP18 data.\n\n\nclim_recal.utils.core\nUtility functions.\n\n\nclim_recal.utils.server\nUtility functions.\n\n\nclim_recal.utils.xarray\n\n\n\nclim_recal.utils.data",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#data-source-management",
    "href": "docs/reference/index.html#data-source-management",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded and processed\n\n\n\nclim_recal.pipeline\nWrappers to automate the entire pipeline.\n\n\nclim_recal.ceda_ftp_download\n\n\n\nclim_recal.data_loader\n\n\n\nclim_recal.config\n\n\n\nclim_recal.resample\nResample UKHADS data and UKCP18 data.\n\n\nclim_recal.utils.core\nUtility functions.\n\n\nclim_recal.utils.server\nUtility functions.\n\n\nclim_recal.utils.xarray\n\n\n\nclim_recal.utils.data",
    "crumbs": [
      "API Reference"
    ]
  }
]